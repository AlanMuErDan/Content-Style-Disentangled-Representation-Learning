#!/usr/bin/env python3
"""
å›¾ç‰‡ç›¸ä¼¼åº¦æ£€æŸ¥å™¨ - ä½¿ç”¨è®­ç»ƒå¥½çš„Siameseç½‘ç»œåˆ¤æ–­ä¸¤å¼ å›¾ç‰‡æ˜¯å¦ç›¸ä¼¼
ä½œè€…: gz2199
ç”¨é€”: ç›´æ¥è¾“å…¥ä¸¤å¼ å›¾ç‰‡ï¼Œé€šè¿‡Siameseç½‘ç»œåˆ¤æ–­ç›¸ä¼¼åº¦
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from PIL import Image
import torchvision.transforms as transforms
import numpy as np
import argparse
import os
from typing import Tuple, Optional, Union

# å¤åˆ¶æ¨¡å‹å®šä¹‰ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
class EnhancedContentEncoder(nn.Module):
    """å¢å¼ºçš„å†…å®¹ç¼–ç å™¨ - æ›´æ·±çš„ç½‘ç»œ"""
    def __init__(self, in_ch=1, emb_dim=512):
        super().__init__()
        
        # æ›´æ·±çš„å·ç§¯ç½‘ç»œ - å—VGGå¯å‘çš„è®¾è®¡
        self.features = nn.Sequential(
            # Block 1 - 64 channels
            nn.Conv2d(in_ch, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            
            # Block 2 - 128 channels  
            nn.Conv2d(64, 128, 3, 2, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),  # 32â†’16
            nn.Conv2d(128, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            
            # Block 3 - 256 channels
            nn.Conv2d(128, 256, 3, 2, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),  # 16â†’8
            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            
            # Block 4 - 512 channels
            nn.Conv2d(256, 512, 3, 2, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),  # 8â†’4
            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            
            # Block 5 - æ·±å±‚ç‰¹å¾
            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Dropout(0.2)
        )
        
        self.spatial_pool = nn.AdaptiveAvgPool2d(4)  # ä¿ç•™4x4ç©ºé—´ä¿¡æ¯
        
        self.classifier = nn.Sequential(
            nn.Linear(512 * 16, emb_dim * 2),
            nn.BatchNorm1d(emb_dim * 2),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(emb_dim * 2, emb_dim),
            nn.BatchNorm1d(emb_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2)
        )

    def forward(self, x):
        if x.ndim == 3:
            x = x.unsqueeze(0)
        if x.size(1) not in (1,3):
            x = x.mean(dim=1, keepdim=True)
        
        h = self.features(x)
        h = self.spatial_pool(h)
        h = h.view(h.size(0), -1)
        return self.classifier(h)

class EnhancedStyleEncoder(nn.Module):
    """å¢å¼ºçš„é£æ ¼ç¼–ç å™¨ - VGGé£æ ¼çš„å¤šå±‚ç‰¹å¾"""
    def __init__(self, in_ch=1, emb_dim=512):
        super().__init__()
        
        # VGGé£æ ¼çš„ç‰¹å¾æå–å™¨
        self.features = nn.Sequential(
            # Block 1
            nn.Conv2d(in_ch, 32, 3, 2, 1), nn.BatchNorm2d(32), nn.ReLU(inplace=True),
            nn.Conv2d(32, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            
            # Block 2
            nn.Conv2d(64, 128, 3, 2, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            
            # Block 3
            nn.Conv2d(128, 256, 3, 2, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            
            # Block 4
            nn.Conv2d(256, 512, 3, 2, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Dropout(0.3)
        )
        
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        
        self.classifier = nn.Sequential(
            nn.Linear(512, emb_dim * 2),
            nn.BatchNorm1d(emb_dim * 2),
            nn.ReLU(inplace=True),
            nn.Dropout(0.4),
            nn.Linear(emb_dim * 2, emb_dim),
            nn.BatchNorm1d(emb_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3)
        )

    def forward(self, x):
        if x.ndim == 3:
            x = x.unsqueeze(0)
        if x.size(1) not in (1,3):
            x = x.mean(dim=1, keepdim=True)
        
        h = self.features(x)
        h = self.global_pool(h)
        h = h.view(h.size(0), -1)
        return self.classifier(h)

class SiameseJudge(nn.Module):
    """Siameseç½‘ç»œ - ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´"""
    def __init__(self, in_ch=1, emb_dim=512, mlp_hidden=512, task="content", encoder_type="enhanced"):
        super().__init__()
        
        # ğŸ”¥ æ”¯æŒå¤šç§ç¼–ç å™¨ç±»å‹
        if encoder_type == "enhanced":
            # å¢å¼ºç‰ˆç¼–ç å™¨ - VGGé£æ ¼çš„æ·±å±‚ç½‘ç»œ
            if task == "content":
                self.encoder = EnhancedContentEncoder(in_ch=in_ch, emb_dim=emb_dim)
            else:  # style
                self.encoder = EnhancedStyleEncoder(in_ch=in_ch, emb_dim=emb_dim)
        else:
            raise ValueError(f"Unsupported encoder_type: {encoder_type}")
            
        # æ›´æ·±çš„åˆ†ç±»å¤´
        self.head = nn.Sequential(
            nn.Linear(emb_dim, mlp_hidden), 
            nn.BatchNorm1d(mlp_hidden),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(mlp_hidden, mlp_hidden//2),
            nn.BatchNorm1d(mlp_hidden//2),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(mlp_hidden//2, mlp_hidden//4),
            nn.BatchNorm1d(mlp_hidden//4),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(mlp_hidden//4, 1)
        )

    def forward(self, x1, x2):
        v1 = self.encoder(x1)
        v2 = self.encoder(x2)
        diff = torch.abs(v1 - v2)
        logit = self.head(diff)
        return torch.sigmoid(logit).squeeze()  # ç›´æ¥è¿”å›æ¦‚ç‡

class ImageSimilarityChecker:
    """å›¾ç‰‡ç›¸ä¼¼åº¦æ£€æŸ¥å™¨"""
    
    def __init__(self, 
                 content_model_path: str = "10kcontent_siamese_model_full.pth",
                 style_model_path: str = "10kstyle_siamese_model_full.pth",
                 device: str = "auto"):
        """
        åˆå§‹åŒ–ç›¸ä¼¼åº¦æ£€æŸ¥å™¨
        
        Args:
            content_model_path: å†…å®¹æ¨¡å‹è·¯å¾„
            style_model_path: é£æ ¼æ¨¡å‹è·¯å¾„
            device: è®¾å¤‡ ("auto", "cuda", "cpu")
        """
        self.device = self._get_device(device)
        
        # å›¾åƒé¢„å¤„ç†
        self.transform = transforms.Compose([
            transforms.Resize((64, 64)),  # è°ƒæ•´åˆ°è®­ç»ƒæ—¶çš„å°ºå¯¸
            transforms.Grayscale(num_output_channels=1),  # è½¬ä¸ºç°åº¦å›¾
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5], std=[0.5])  # å½’ä¸€åŒ–åˆ°[-1, 1]
        ])
        
        # åŠ è½½æ¨¡å‹
        self.content_model = self._load_model(content_model_path, "content")
        self.style_model = self._load_model(style_model_path, "style")
        
        print(f"âœ… ç›¸ä¼¼åº¦æ£€æŸ¥å™¨åˆå§‹åŒ–å®Œæˆ (è®¾å¤‡: {self.device})")
    
    def _get_device(self, device: str) -> torch.device:
        """è·å–è®¡ç®—è®¾å¤‡"""
        if device == "auto":
            return torch.device("cuda" if torch.cuda.is_available() else "cpu")
        return torch.device(device)
    
    def _load_model(self, model_path: str, task: str) -> SiameseJudge:
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        if not os.path.exists(model_path):
            # å°è¯•è°ƒè¯•æ¨¡å‹è·¯å¾„
            debug_path = f"debug_{task}_siamese_model.pth"
            if os.path.exists(debug_path):
                model_path = debug_path
                print(f"âš ï¸  ä½¿ç”¨è°ƒè¯•æ¨¡å‹: {debug_path}")
            else:
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
        
        # åˆ›å»ºæ¨¡å‹ - ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ¶æ„
        model = SiameseJudge(
            in_ch=1, 
            emb_dim=512, 
            mlp_hidden=512, 
            task=task, 
            encoder_type="enhanced"
        )
        
        # åŠ è½½æƒé‡
        try:
            state_dict = torch.load(model_path, map_location=self.device)
            model.load_state_dict(state_dict)
            print(f"âœ… {task}æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
        except Exception as e:
            print(f"âŒ {task}æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
        
        model.to(self.device)
        model.eval()
        return model
    
    def preprocess_image(self, image_input: Union[str, Image.Image, np.ndarray]) -> torch.Tensor:
        """é¢„å¤„ç†å›¾åƒ"""
        # å¤„ç†ä¸åŒç±»å‹çš„è¾“å…¥
        if isinstance(image_input, str):
            # æ–‡ä»¶è·¯å¾„
            if not os.path.exists(image_input):
                raise FileNotFoundError(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_input}")
            image = Image.open(image_input)
        elif isinstance(image_input, np.ndarray):
            # numpyæ•°ç»„
            image = Image.fromarray(image_input)
        elif isinstance(image_input, Image.Image):
            # PILå›¾åƒ
            image = image_input
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å›¾åƒç±»å‹: {type(image_input)}")
        
        # åº”ç”¨é¢„å¤„ç†
        tensor = self.transform(image)
        return tensor.unsqueeze(0).to(self.device)  # æ·»åŠ batchç»´åº¦
    
    def check_content_similarity(self, 
                                image1: Union[str, Image.Image, np.ndarray],
                                image2: Union[str, Image.Image, np.ndarray],
                                threshold: float = 0.5) -> Tuple[bool, float]:
        """
        æ£€æŸ¥ä¸¤å¼ å›¾ç‰‡çš„å†…å®¹ç›¸ä¼¼åº¦
        
        Args:
            image1: ç¬¬ä¸€å¼ å›¾ç‰‡
            image2: ç¬¬äºŒå¼ å›¾ç‰‡
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            (æ˜¯å¦ç›¸ä¼¼, ç›¸ä¼¼åº¦åˆ†æ•°)
        """
        with torch.no_grad():
            img1_tensor = self.preprocess_image(image1)
            img2_tensor = self.preprocess_image(image2)
            
            similarity_score = self.content_model(img1_tensor, img2_tensor).item()
            is_similar = similarity_score > threshold
            
            return is_similar, similarity_score
    
    def check_style_similarity(self, 
                              image1: Union[str, Image.Image, np.ndarray],
                              image2: Union[str, Image.Image, np.ndarray],
                              threshold: float = 0.5) -> Tuple[bool, float]:
        """
        æ£€æŸ¥ä¸¤å¼ å›¾ç‰‡çš„é£æ ¼ç›¸ä¼¼åº¦
        
        Args:
            image1: ç¬¬ä¸€å¼ å›¾ç‰‡
            image2: ç¬¬äºŒå¼ å›¾ç‰‡
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            (æ˜¯å¦ç›¸ä¼¼, ç›¸ä¼¼åº¦åˆ†æ•°)
        """
        with torch.no_grad():
            img1_tensor = self.preprocess_image(image1)
            img2_tensor = self.preprocess_image(image2)
            
            similarity_score = self.style_model(img1_tensor, img2_tensor).item()
            is_similar = similarity_score > threshold
            
            return is_similar, similarity_score
    
    def comprehensive_check(self, 
                           image1: Union[str, Image.Image, np.ndarray],
                           image2: Union[str, Image.Image, np.ndarray],
                           content_threshold: float = 0.5,
                           style_threshold: float = 0.5) -> dict:
        """
        å…¨é¢æ£€æŸ¥ä¸¤å¼ å›¾ç‰‡çš„ç›¸ä¼¼åº¦ï¼ˆå†…å®¹+é£æ ¼ï¼‰
        
        Args:
            image1: ç¬¬ä¸€å¼ å›¾ç‰‡
            image2: ç¬¬äºŒå¼ å›¾ç‰‡
            content_threshold: å†…å®¹ç›¸ä¼¼åº¦é˜ˆå€¼
            style_threshold: é£æ ¼ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            åŒ…å«è¯¦ç»†ç»“æœçš„å­—å…¸
        """
        # æ£€æŸ¥å†…å®¹ç›¸ä¼¼åº¦
        content_similar, content_score = self.check_content_similarity(
            image1, image2, content_threshold
        )
        
        # æ£€æŸ¥é£æ ¼ç›¸ä¼¼åº¦
        style_similar, style_score = self.check_style_similarity(
            image1, image2, style_threshold
        )
        
        # ç»¼åˆåˆ¤æ–­
        overall_similar = content_similar and style_similar
        overall_score = (content_score + style_score) / 2
        
        return {
            "content": {
                "similar": content_similar,
                "score": content_score,
                "threshold": content_threshold
            },
            "style": {
                "similar": style_similar,
                "score": style_score,
                "threshold": style_threshold
            },
            "overall": {
                "similar": overall_similar,
                "score": overall_score
            }
        }

def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(description="å›¾ç‰‡ç›¸ä¼¼åº¦æ£€æŸ¥å™¨")
    parser.add_argument("image1", help="ç¬¬ä¸€å¼ å›¾ç‰‡è·¯å¾„")
    parser.add_argument("image2", help="ç¬¬äºŒå¼ å›¾ç‰‡è·¯å¾„")
    parser.add_argument("--mode", choices=["content", "style", "both"], 
                       default="both", help="æ£€æŸ¥æ¨¡å¼")
    parser.add_argument("--content-threshold", type=float, default=0.5,
                       help="å†…å®¹ç›¸ä¼¼åº¦é˜ˆå€¼")
    parser.add_argument("--style-threshold", type=float, default=0.5,
                       help="é£æ ¼ç›¸ä¼¼åº¦é˜ˆå€¼")
    parser.add_argument("--content-model", 
                       default="10kcontent_siamese_model_full.pth",
                       help="å†…å®¹æ¨¡å‹è·¯å¾„")
    parser.add_argument("--style-model", 
                       default="10kstyle_siamese_model_full.pth",
                       help="é£æ ¼æ¨¡å‹è·¯å¾„")
    
    args = parser.parse_args()
    
    try:
        # åˆå§‹åŒ–æ£€æŸ¥å™¨
        checker = ImageSimilarityChecker(
            content_model_path=args.content_model,
            style_model_path=args.style_model
        )
        
        print(f"\nğŸ” æ¯”è¾ƒå›¾ç‰‡:")
        print(f"  å›¾ç‰‡1: {args.image1}")
        print(f"  å›¾ç‰‡2: {args.image2}")
        print(f"  æ¨¡å¼: {args.mode}")
        print("=" * 60)
        
        if args.mode == "content":
            # åªæ£€æŸ¥å†…å®¹ç›¸ä¼¼åº¦
            is_similar, score = checker.check_content_similarity(
                args.image1, args.image2, args.content_threshold
            )
            print(f"ğŸ“ å†…å®¹ç›¸ä¼¼åº¦: {score:.4f}")
            print(f"   é˜ˆå€¼: {args.content_threshold}")
            print(f"   ç»“æœ: {'âœ… ç›¸ä¼¼' if is_similar else 'âŒ ä¸ç›¸ä¼¼'}")
            
        elif args.mode == "style":
            # åªæ£€æŸ¥é£æ ¼ç›¸ä¼¼åº¦
            is_similar, score = checker.check_style_similarity(
                args.image1, args.image2, args.style_threshold
            )
            print(f"ğŸ¨ é£æ ¼ç›¸ä¼¼åº¦: {score:.4f}")
            print(f"   é˜ˆå€¼: {args.style_threshold}")
            print(f"   ç»“æœ: {'âœ… ç›¸ä¼¼' if is_similar else 'âŒ ä¸ç›¸ä¼¼'}")
            
        else:  # both
            # å…¨é¢æ£€æŸ¥
            result = checker.comprehensive_check(
                args.image1, args.image2, 
                args.content_threshold, args.style_threshold
            )
            
            print(f"ğŸ“ å†…å®¹ç›¸ä¼¼åº¦: {result['content']['score']:.4f}")
            print(f"   é˜ˆå€¼: {result['content']['threshold']}")
            print(f"   ç»“æœ: {'âœ… ç›¸ä¼¼' if result['content']['similar'] else 'âŒ ä¸ç›¸ä¼¼'}")
            print()
            print(f"ğŸ¨ é£æ ¼ç›¸ä¼¼åº¦: {result['style']['score']:.4f}")
            print(f"   é˜ˆå€¼: {result['style']['threshold']}")
            print(f"   ç»“æœ: {'âœ… ç›¸ä¼¼' if result['style']['similar'] else 'âŒ ä¸ç›¸ä¼¼'}")
            print()
            print(f"ğŸ¯ ç»¼åˆç›¸ä¼¼åº¦: {result['overall']['score']:.4f}")
            print(f"   ç»¼åˆç»“æœ: {'âœ… ç›¸ä¼¼' if result['overall']['similar'] else 'âŒ ä¸ç›¸ä¼¼'}")
        
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())