#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Full dataset training version - ä½¿ç”¨å®Œæ•´æ•°æ®é›†è¿›è¡Œè®­ç»ƒ
"""
import os
import random
import numpy as np
import pickle
from dataclasses import dataclass
from typing import Any, Tuple, Literal, Optional
import sys
import yaml

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from huggingface_hub import hf_hub_download
import torchvision.models as models
from torchvision import transforms
from PIL import Image

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append('/scratch/gz2199/Content-Style-Disentangled-Representation-Learning/Content-Style-Disentangled-Representation-Learning')
from models import build_encoder, build_decoder

# å¯¼å…¥æœ¬åœ°çš„font_datasetæ¨¡å—ï¼ˆPTç‰ˆæœ¬ï¼‰
sys.path.append('/scratch/gz2199/Content-Style-Disentangled-Representation-Learning/0/1/0')
from font_dataset import FourWayFontPairLatentPTDataset

# å¯¼å…¥åŸæœ‰çš„ç»„ä»¶ - å¤åˆ¶è€Œä¸æ˜¯å¯¼å…¥é¿å…è·¯å¾„é—®é¢˜
import math
import json

# å¤åˆ¶å¿…è¦çš„å‡½æ•°å’Œç±»
def load_latents_from_hub(
    repo_id: str = "YuanhengLi/Font-Latent-Full-PT",
    filename: str = "font_latents_v2.pt",
    token: Optional[str] = os.getenv("HF_TOKEN"),
    map_location: str = "cpu",
    local_path: Optional[str] = None,
):
    if local_path and os.path.exists(local_path):
        print(f"[INFO] Loading from local file: {local_path}")
        obj = torch.load(local_path, map_location=map_location)
        print(f"[INFO] Loaded from local file")
        return obj
    
    try:
        pt_path = hf_hub_download(repo_id=repo_id, filename=filename, token=token)
        obj = torch.load(pt_path, map_location=map_location)
        print(f"[INFO] Loaded from {repo_id}/{filename}")
        return obj
    except Exception as e:
        print(f"[ERROR] Failed to load from Hub: {e}")
        raise

class ImprovedLatentAccessor:
    """
    ä½¿ç”¨FourWayFontPairLatentPTDatasetçš„æ”¹è¿›ç‰ˆLatentAccessor
    è‡ªåŠ¨æ¨æ–­å®é™…çš„å­—ä½“å’Œå­—ç¬¦æ•°é‡ï¼Œä¸ä¾èµ–ç¡¬ç¼–ç å‡è®¾
    """
    def __init__(self, 
                 pt_path: str, 
                 chars_path: Optional[str] = None,
                 fonts_json: Optional[str] = None,
                 device='cpu',
                 latent_shape: Tuple[int, int, int] = (4, 16, 16)):
        self.device = device
        self.pt_path = pt_path
        
        # ä¼˜å…ˆå°è¯•ä½¿ç”¨FourWayFontPairLatentPTDataset
        if chars_path and fonts_json and os.path.exists(chars_path) and os.path.exists(fonts_json):
            print("[INFO] ä½¿ç”¨FourWayFontPairLatentPTDatasetï¼ˆæ¨èæ–¹å¼ï¼‰...")
            try:
                self.dataset = FourWayFontPairLatentPTDataset(
                    pt_path=pt_path,
                    chars_path=chars_path,
                    fonts_json=fonts_json,
                    latent_shape=latent_shape,
                    pair_num=1  # åªéœ€è¦æ¨æ–­ç»“æ„ï¼Œä¸éœ€è¦å¤§é‡æ•°æ®
                )
                
                # è·å–å®é™…çš„å­—ä½“å’Œå­—ç¬¦ä¿¡æ¯
                self.fonts = self.dataset.fonts
                self.chars = self.dataset.chars
                self.common_chars = self.dataset.common_chars
                self.num_fonts = self.dataset.n
                self.num_characters = self.dataset.m
                
                # ğŸ”¥ æ·»åŠ å…¼å®¹æ€§å±æ€§
                self.num_styles_per_char = self.num_fonts  # åœ¨æ–°ç»“æ„ä¸­ï¼Œæ¯ä¸ªå­—ç¬¦æœ‰num_fontsç§é£æ ¼
                
                print(f"[ImprovedLatentAccessor] ä½¿ç”¨FourWayDatasetå‘ç°:")
                print(f"  - å­—ä½“æ•°é‡: {self.num_fonts}")
                print(f"  - å­—ç¬¦æ•°é‡: {self.num_characters}")
                print(f"  - æ¯å­—ç¬¦é£æ ¼æ•°: {self.num_styles_per_char}")
                print(f"  - æ€»å¯èƒ½ç»„åˆ: {self.num_fonts * self.num_characters}")
                print(f"  - æ•°æ®ç»„ç»‡: font_idx * {self.num_characters} + char_idx")
                
                # ä¿å­˜æ•°æ®ç»“æ„æ˜ å°„
                self.font_to_idx = {font: i for i, font in enumerate(self.fonts)}
                self.char_to_idx = {char: i for i, char in enumerate(self.chars)}
                self.fallback_mode = False
                return
            except Exception as e:
                print(f"[WARNING] FourWayFontPairLatentPTDatasetåˆå§‹åŒ–å¤±è´¥: {e}")
                print("[INFO] é™çº§åˆ°fallbackæ¨¡å¼...")
        else:
            missing_files = []
            if not chars_path:
                missing_files.append("chars_path")
            elif not os.path.exists(chars_path):
                missing_files.append(f"chars_path({chars_path})")
            if not fonts_json:
                missing_files.append("fonts_json")
            elif not os.path.exists(fonts_json):
                missing_files.append(f"fonts_json({fonts_json})")
            
            print(f"[INFO] ç¼ºå°‘æ–‡ä»¶: {', '.join(missing_files)}")
            print("[INFO] ä½¿ç”¨fallbackæ¨¡å¼ï¼Œé»˜è®¤2056å­—ä½“Ã—4574å­—ç¬¦ç»“æ„...")
        
        # Fallbackæ¨¡å¼ï¼šä½¿ç”¨2056å­—ä½“Ã—4574å­—ç¬¦çš„é»˜è®¤ç»“æ„
        self._create_from_old_accessor(pt_path, latent_shape, device)
        
    def _create_from_old_accessor(self, pt_path: str, latent_shape: Tuple[int, int, int], device: str):
        """ä½¿ç”¨æ—§çš„LatentAccessoré€»è¾‘ä½œä¸ºfallback"""
        print("[INFO] ä½¿ç”¨fallbackæ¨¡å¼ï¼Œé‡‡ç”¨2056å­—ä½“Ã—4574å­—ç¬¦ç»“æ„...")
        
        # åŠ è½½PTæ–‡ä»¶
        blob = torch.load(pt_path, map_location="cpu")
        if isinstance(blob, dict) and "latents" in blob:
            latents = blob["latents"]
        else:
            latents = blob
            
        if isinstance(latents, torch.Tensor):
            if latents.dim() == 4:  # (N, H, W, C)
                self.total_samples = latents.shape[0]
                self.latents_hwc = latents
            else:
                self.total_samples = latents.shape[0]
                self.raw_tensor = latents
                
            # ä½¿ç”¨å‡†ç¡®çš„æ•°æ®ç»“æ„: 2056å­—ä½“ Ã— 4574å­—ç¬¦
            self.num_characters = 4574  # å­—ç¬¦æ•°é‡
            self.num_fonts = 2056       # å­—ä½“æ•°é‡
            
            # éªŒè¯æ•°æ®æ€»é‡æ˜¯å¦åŒ¹é…
            expected_total = self.num_fonts * self.num_characters
            if self.total_samples != expected_total:
                print(f"[WARNING] æ•°æ®é‡ä¸åŒ¹é…!")
                print(f"  - å®é™…æ ·æœ¬: {self.total_samples}")
                print(f"  - é¢„æœŸæ ·æœ¬: {expected_total} (2056Ã—4574)")
                print(f"  - å°†æŒ‰å®é™…æ•°æ®è°ƒæ•´ç»“æ„...")
                
                # å°è¯•å…¶ä»–å¯èƒ½çš„ç»„ç»‡æ–¹å¼
                if self.total_samples % 4574 == 0:
                    self.num_fonts = self.total_samples // 4574
                    print(f"  - è°ƒæ•´ä¸º: {self.num_fonts}å­—ä½“ Ã— {self.num_characters}å­—ç¬¦")
                elif self.total_samples % 2056 == 0:
                    self.num_characters = self.total_samples // 2056
                    print(f"  - è°ƒæ•´ä¸º: {self.num_fonts}å­—ä½“ Ã— {self.num_characters}å­—ç¬¦")
                else:
                    # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œä½¿ç”¨å¹³æ–¹æ ¹è¿‘ä¼¼
                    import math
                    side = int(math.sqrt(self.total_samples))
                    self.num_fonts = side
                    self.num_characters = self.total_samples // side
                    print(f"  - è¿‘ä¼¼è°ƒæ•´ä¸º: {self.num_fonts}å­—ä½“ Ã— {self.num_characters}å­—ç¬¦")
                    
            self.num_styles_per_char = self.num_fonts
            self.fallback_mode = True
            
            print(f"[ImprovedLatentAccessor] Fallbackæ¨¡å¼é…ç½®:")
            print(f"  - å­—ä½“æ•°é‡: {self.num_fonts}")
            print(f"  - å­—ç¬¦æ•°é‡: {self.num_characters}")
            print(f"  - æ•°æ®ç»„ç»‡: font_idx * {self.num_characters} + char_idx")
            
    def get_by_indices(self, font_idx: int, char_idx: int) -> torch.Tensor:
        """æ ¹æ®å­—ä½“å’Œå­—ç¬¦ç´¢å¼•è·å–æ½œåœ¨å‘é‡"""
        if hasattr(self, 'dataset') and not self.fallback_mode:
            # ä½¿ç”¨FourWayDatasetæ–¹å¼
            return self.dataset.get_latent_by_indices(font_idx, char_idx).to(self.device)
        else:
            # ä½¿ç”¨fallbackæ–¹å¼
            linear_idx = font_idx * self.num_characters + char_idx
            
            if hasattr(self, 'latents_hwc'):
                # 4Dæ ¼å¼ (N, H, W, C)
                latent_hwc = self.latents_hwc[linear_idx]  # (H, W, C)
                latent_chw = latent_hwc.permute(2, 0, 1)   # (C, H, W)
                return latent_chw.to(self.device)
            elif hasattr(self, 'raw_tensor'):
                return self.raw_tensor[linear_idx].to(self.device)
            else:
                raise RuntimeError("No latent data available")
    
    def get_by_names(self, font_name: str, char: str) -> torch.Tensor:
        """æ ¹æ®å­—ä½“åç§°å’Œå­—ç¬¦è·å–æ½œåœ¨å‘é‡ï¼ˆä»…FourWayæ¨¡å¼æ”¯æŒï¼‰"""
        if hasattr(self, 'dataset') and not self.fallback_mode:
            font_idx = self.font_to_idx.get(font_name)
            char_idx = self.char_to_idx.get(char)
            
            if font_idx is None:
                raise ValueError(f"Unknown font: {font_name}")
            if char_idx is None:
                raise ValueError(f"Unknown character: {char}")
                
            return self.get_by_indices(font_idx, char_idx)
        else:
            raise NotImplementedError("get_by_names only available in FourWay mode")
    
    def random_sample(self) -> Tuple[torch.Tensor, int, int]:
        """éšæœºé‡‡æ ·ä¸€ä¸ªæ½œåœ¨å‘é‡"""
        font_idx = random.randint(0, self.num_fonts - 1)
        char_idx = random.randint(0, self.num_characters - 1)
        return self.get_by_indices(font_idx, char_idx), font_idx, char_idx
        
    def get(self, content_i: int, style_p: int) -> torch.Tensor:
        """å…¼å®¹åŸæ¥å£çš„æ–¹æ³•"""
        # content_iå¯¹åº”å­—ç¬¦ç´¢å¼•ï¼Œstyle_på¯¹åº”å­—ä½“ç´¢å¼•
        return self.get_by_indices(style_p % self.num_fonts, content_i % self.num_characters)

# VAEè§£ç å™¨ç›¸å…³å‡½æ•° - ä½¿ç”¨vae_io.pyçš„å®ç°
def _load_config(path: str) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(path, "r") as f:
        cfg = yaml.safe_load(f)
    return cfg["vae"] if "vae" in cfg else cfg

def load_vae_decoder(
    config_path: str = "/scratch/gz2199/Content-Style-Disentangled-Representation-Learning/Content-Style-Disentangled-Representation-Learning/configs/config.yaml",
    ckpt_path: str = "/scratch/gz2199/Content-Style-Disentangled-Representation-Learning/0/1/0/vae_best_ckpt.pth",
    device: str = "cuda"
) -> Tuple[torch.nn.Module, dict, torch.device]:
    """
    åŠ è½½VAEè§£ç å™¨
    
    Returns:
        decoder  â€“ torch.nn.Module (eval mode)
        cfg      â€“ dict (é…ç½®)
        device   â€“ torch.device
    """
    device = torch.device(device if torch.cuda.is_available() else "cpu")
    cfg = _load_config(config_path)

    decoder = build_decoder(
        name=cfg["decoder"],
        img_size=cfg["img_size"],
        latent_channels=cfg.get("latent_channels", 4),
    ).to(device).eval()

    # åŠ è½½æƒé‡
    ckpt = torch.load(ckpt_path, map_location=device)
    if "decoder" in ckpt:
        decoder.load_state_dict(ckpt["decoder"])
    else:
        # å¦‚æœcheckpointç›´æ¥æ˜¯decoderçŠ¶æ€
        decoder.load_state_dict(ckpt)

    print(f"[INFO] Loaded VAE decoder on {device}")
    return decoder, cfg, device

@torch.no_grad()
def decode_to_image(decoder: nn.Module, z: torch.Tensor, cfg: dict, device: torch.device) -> torch.Tensor:
    """
    ä½¿ç”¨VAEè§£ç å™¨å°†æ½œåœ¨ç¼–ç è§£ç ä¸ºå›¾åƒ
    
    Args:
        decoder: VAEè§£ç å™¨
        z: æ½œåœ¨ç¼–ç  [C,H,W] æˆ–å…¶ä»–æ ¼å¼
        cfg: é…ç½®å­—å…¸
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        img: å›¾åƒå¼ é‡ [C,H,W] èŒƒå›´[0,1]
    """
    # ğŸ”¥ æ£€æŸ¥å¹¶ä¿®å¤æ•°æ®ç±»å‹ä¸åŒ¹é…é—®é¢˜ï¼ˆå‡å°‘æ—¥å¿—è¾“å‡ºï¼‰
    decoder_dtype = next(decoder.parameters()).dtype
    if z.dtype != decoder_dtype:
        # åªåœ¨ç¬¬ä¸€æ¬¡è½¬æ¢æ—¶è¾“å‡ºä¿¡æ¯ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
        if not hasattr(decode_to_image, '_conversion_logged'):
            print(f"[INFO] è‡ªåŠ¨è½¬æ¢æ½œåœ¨ç¼–ç ç±»å‹: {z.dtype} -> {decoder_dtype}")
            decode_to_image._conversion_logged = True
        z = z.to(dtype=decoder_dtype)
    
    # ç¡®ä¿æ½œåœ¨ç¼–ç æ ¼å¼æ­£ç¡®
    if z.dim() == 2:
        # å¦‚æœæ˜¯2Dï¼Œå°è¯•é‡å¡‘ä¸º3D
        if z.shape == (16, 4):
            z = z.view(4, 4, 4)  # [C, H, W]
        else:
            raise ValueError(f"Unexpected 2D tensor shape: {z.shape}")
    elif z.dim() == 1:
        # å¦‚æœæ˜¯1Dï¼Œé‡å¡‘ä¸º3D
        if z.numel() == 64:  # 4*4*4
            z = z.view(4, 4, 4)
        else:
            raise ValueError(f"Unexpected 1D tensor size: {z.numel()}")
    elif z.dim() == 3:
        # ğŸ”¥ 3Då¼ é‡ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°æ’åˆ—ç»´åº¦
        if z.shape == (16, 16, 4):
            z = z.permute(2, 0, 1)  # [H, W, C] -> [C, H, W]
            if not hasattr(decode_to_image, '_reshape_logged'):
                print(f"[INFO] é‡æ–°æ’åˆ—ç»´åº¦: (16,16,4) -> {z.shape}")
                decode_to_image._reshape_logged = True
    elif z.dim() != 3:
        raise ValueError(f"Latent must be 1D, 2D, or 3D, got {z.dim()}D")
    
    # æ·»åŠ batchç»´åº¦å¹¶ç§»åˆ°è®¾å¤‡ï¼Œç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
    z_batch = z.unsqueeze(0).to(device=device, dtype=decoder_dtype)  # [1, C, H, W]
    
    # è§£ç 
    with torch.no_grad():
        recon = decoder(z_batch).squeeze(0).cpu()  # [C, H, W]
    
    # ç¡®ä¿è¾“å‡ºåœ¨[0,1]èŒƒå›´
    recon = torch.clamp(recon, 0, 1)
    
    return recon

class TinyEncoder(nn.Module):
    def __init__(self, in_ch=1, emb_dim=512):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(in_ch, 64, 3, 2, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, 3, 2, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, 3, 2, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.Conv2d(256, 512, 3, 2, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, 2, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d(1),
            nn.Dropout(0.3)
        )
        self.fc = nn.Sequential(
            nn.Linear(512, emb_dim),
            nn.BatchNorm1d(emb_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3)
        )

    def forward(self, x):
        if x.ndim == 3:
            x = x.unsqueeze(0)
        if x.size(1) not in (1,3):
            x = x.mean(dim=1, keepdim=True)
        h = self.net(x)
        h = h.view(h.size(0), -1)
        return self.fc(h)

class SiameseJudge(nn.Module):
    def __init__(self, in_ch=1, emb_dim=512, mlp_hidden=512):
        super().__init__()
        self.encoder = TinyEncoder(in_ch=in_ch, emb_dim=emb_dim)
        self.head = nn.Sequential(
            nn.Linear(emb_dim, mlp_hidden), 
            nn.BatchNorm1d(mlp_hidden),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(mlp_hidden, mlp_hidden//2),
            nn.BatchNorm1d(mlp_hidden//2),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(mlp_hidden//2, 1)
        )

    def forward(self, x1, x2):
        v1 = self.encoder(x1)
        v2 = self.encoder(x2)
        diff = torch.abs(v1 - v2)
        logit = self.head(diff)
        return logit, v1, v2

@torch.no_grad()
def score_pair(model: nn.Module, img1: torch.Tensor, img2: torch.Tensor, device=None) -> float:
    device = device or next(model.parameters()).device
    model.eval()
    img1 = img1.unsqueeze(0).to(device)
    img2 = img2.unsqueeze(0).to(device)
    logit, _, _ = model(img1, img2)
    prob = torch.sigmoid(logit).item()
    return prob

class FullDatasetPairDataset(Dataset):
    """
    ä½¿ç”¨å®Œæ•´æ•°æ®é›†çš„é…å¯¹æ•°æ®é›†
    ä»å¤§é‡æ ·æœ¬ä¸­éšæœºé€‰æ‹©é…å¯¹è¿›è¡Œè®­ç»ƒ
    """
    def __init__(self, 
                 accessor: ImprovedLatentAccessor, 
                 decoder: nn.Module,
                 cfg: dict,
                 device: torch.device,
                 task: Literal["content","style"]="content",
                 num_styles: int = 100,
                 num_contents: int = 1000,
                 length: int = 50000,  # å¢åŠ è®­ç»ƒæ ·æœ¬æ•°
                 augment: bool = True):
        """
        Args:
            accessor: æ”¹è¿›çš„æ½œåœ¨ç¼–ç è®¿é—®å™¨
            decoder: VAEè§£ç å™¨æ¨¡å‹
            cfg: VAEé…ç½®å­—å…¸
            device: è®¡ç®—è®¾å¤‡
            task: ä»»åŠ¡ç±»å‹ ('content' æˆ– 'style')
            num_styles: ä½¿ç”¨çš„é£æ ¼æ•°é‡
            num_contents: ä½¿ç”¨çš„å†…å®¹æ•°é‡  
            length: è®­ç»ƒæ ·æœ¬æ€»æ•°
            augment: æ˜¯å¦ä½¿ç”¨æ•°æ®å¢å¼º
        """
        self.accessor = accessor
        self.decoder = decoder
        self.cfg = cfg
        self.device = device
        self.task = task
        self.num_styles = num_styles
        self.num_contents = num_contents
        self.length = length
        self.augment = augment
        
        print(f"[INFO] åˆ›å»ºå…¨æ•°æ®é›†è®­ç»ƒå™¨:")
        print(f"  - ä»»åŠ¡ç±»å‹: {task}")
        print(f"  - é£æ ¼æ•°é‡: {num_styles}")
        print(f"  - å†…å®¹æ•°é‡: {num_contents}")
        print(f"  - è®­ç»ƒæ ·æœ¬: {length}")
        
        # é¢„å…ˆæ£€æŸ¥æ•°æ®è®¿é—®æ˜¯å¦æ­£å¸¸
        try:
            test_sample = self.accessor.get(0, 0)
            print(f"  - æ ·æœ¬å½¢çŠ¶: {test_sample.shape}")
        except Exception as e:
            print(f"  - è­¦å‘Š: æ•°æ®è®¿é—®æµ‹è¯•å¤±è´¥: {e}")

    def __len__(self):
        return self.length

    def _aug(self, x: torch.Tensor) -> torch.Tensor:
        """æ•°æ®å¢å¼º"""
        if not self.augment: 
            return x
        # ç®€å•çš„å‡ ä½•å’Œå™ªå£°å¢å¼º
        if random.random() < 0.5:
            x = torch.flip(x, dims=[2])  # æ°´å¹³ç¿»è½¬
        if random.random() < 0.2:
            x = x + torch.randn_like(x) * 0.02
            x = torch.clamp(x, 0.0, 1.0)
        return x

    def __getitem__(self, idx):
        """è·å–è®­ç»ƒæ ·æœ¬å¯¹"""
        # éšæœºé€‰æ‹©ç´¢å¼•
        content_i = random.randint(0, self.num_contents - 1)
        content_j = random.randint(0, self.num_contents - 1)
        style_p = random.randint(0, self.num_styles - 1)
        style_q = random.randint(0, self.num_styles - 1)
        
        # ç¡®ä¿æœ‰ä¸åŒçš„é€‰æ‹©ç”¨äºè´Ÿæ ·æœ¬
        while content_j == content_i:
            content_j = random.randint(0, self.num_contents - 1)
        while style_q == style_p:
            style_q = random.randint(0, self.num_styles - 1)
        
        try:
            # è·å–æ½œåœ¨ç¼–ç 
            z1 = self.accessor.get(content_i, style_p)
            if idx < 5:  # åªåœ¨å‰å‡ ä¸ªæ ·æœ¬æ‰“å°è°ƒè¯•ä¿¡æ¯
                print(f"[DEBUG] Sample {idx}: z1.shape = {z1.shape}")
            
            # è§£ç ä¸ºå›¾åƒ (ä¿æŒåœ¨CPUä¸Š)
            ci_sp = decode_to_image(self.decoder, z1, self.cfg, self.device)
            if idx < 5:
                print(f"[DEBUG] Sample {idx}: ci_sp.shape = {ci_sp.shape}")
            
            # 50/50 æ­£è´Ÿæ ·æœ¬
            is_positive = (random.random() < 0.5)
            
            if self.task == "content":
                if is_positive:
                    # æ­£æ ·æœ¬: ç›¸åŒå†…å®¹ï¼Œä¸åŒé£æ ¼
                    z2 = self.accessor.get(content_i, style_q)
                    ci_sq = decode_to_image(self.decoder, z2, self.cfg, self.device)
                    x1, x2, y = ci_sp, ci_sq, 1.0
                else:
                    # è´Ÿæ ·æœ¬: ä¸åŒå†…å®¹ï¼Œç›¸åŒé£æ ¼
                    z2 = self.accessor.get(content_j, style_p)
                    cj_sp = decode_to_image(self.decoder, z2, self.cfg, self.device)
                    x1, x2, y = ci_sp, cj_sp, 0.0
            else:  # style task
                if is_positive:
                    # æ­£æ ·æœ¬: ç›¸åŒé£æ ¼ï¼Œä¸åŒå†…å®¹
                    z2 = self.accessor.get(content_j, style_p)
                    cj_sp = decode_to_image(self.decoder, z2, self.cfg, self.device)
                    x1, x2, y = ci_sp, cj_sp, 1.0
                else:
                    # è´Ÿæ ·æœ¬: ä¸åŒé£æ ¼ï¼Œç›¸åŒå†…å®¹
                    z2 = self.accessor.get(content_i, style_q)
                    ci_sq = decode_to_image(self.decoder, z2, self.cfg, self.device)
                    x1, x2, y = ci_sp, ci_sq, 0.0

            # åº”ç”¨æ•°æ®å¢å¼º
            x1 = self._aug(x1)
            x2 = self._aug(x2)
            
            # ç¡®ä¿å¼ é‡æ˜¯è¿ç»­çš„å¹¶ä¸”å¯ä»¥è¢«å¤åˆ¶
            x1 = x1.contiguous().detach().clone()
            x2 = x2.contiguous().detach().clone()
            
            return x1, x2, torch.tensor([y], dtype=torch.float32)
            
        except Exception as e:
            print(f"[WARN] æ ·æœ¬ç”Ÿæˆå¤±è´¥ (idx={idx}): {e}")
            # è¿”å›ä¸€ä¸ªdummyæ ·æœ¬é¿å…è®­ç»ƒä¸­æ–­
            dummy_img = torch.zeros(1, 32, 32)
            return dummy_img, dummy_img, torch.tensor([0.0], dtype=torch.float32)

def run_full_training(
    task: Literal["content","style"]="content",
    local_path: Optional[str] = None,
    device="cuda" if torch.cuda.is_available() else "cpu",
    
    # è®­ç»ƒå‚æ•°
    num_styles: int = 500,      # ä½¿ç”¨çš„é£æ ¼æ•°é‡
    num_contents: int = 2000,   # ä½¿ç”¨çš„å†…å®¹æ•°é‡
    train_samples: int = 100000, # è®­ç»ƒæ ·æœ¬æ•°
    batch_size: int = 64,       # æ‰¹é‡å¤§å°
    epochs: int = 20,           # è®­ç»ƒè½®æ•°
    lr: float = 1e-4,           # å­¦ä¹ ç‡
    
    # è¯„ä¼°å‚æ•°
    eval_samples: int = 1000,   # è¯„ä¼°æ ·æœ¬æ•°
):
    """
    è¿è¡Œå®Œæ•´æ•°æ®é›†è®­ç»ƒ
    """
    print("ğŸš€ å¼€å§‹å…¨æ•°æ®é›†å­—ä½“è®­ç»ƒ...")
    print(f"ğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"  - ä»»åŠ¡: {task}")
    print(f"  - è®¾å¤‡: {device}")
    print(f"  - é£æ ¼æ•°: {num_styles}")
    print(f"  - å†…å®¹æ•°: {num_contents}")
    print(f"  - è®­ç»ƒæ ·æœ¬: {train_samples}")
    print(f"  - æ‰¹é‡å¤§å°: {batch_size}")
    print(f"  - è®­ç»ƒè½®æ•°: {epochs}")
    
    # 1. åŠ è½½æ•°æ®
    print("\nğŸ“¥ åŠ è½½æ½œåœ¨ç¼–ç æ•°æ®...")
    # ä½¿ç”¨ImprovedLatentAccessoræ›¿ä»£åŸæ¥çš„LatentAccessor
    accessor = ImprovedLatentAccessor(
        pt_path=local_path,
        chars_path="/scratch/gz2199/Content-Style-Disentangled-Representation-Learning/0/1/0/char_list.txt",
        fonts_json="/scratch/gz2199/Content-Style-Disentangled-Representation-Learning/0/1/0/lmdb_keys.json",
        device="cpu"
    )
    
    # 2. åŠ è½½VAEè§£ç å™¨
    print("\nğŸ”§ åŠ è½½VAEè§£ç å™¨...")
    decoder, cfg, device_used = load_vae_decoder(device=device)
    
    # 3. åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
    print("\nğŸ“š åˆ›å»ºè®­ç»ƒæ•°æ®é›†...")
    train_dataset = FullDatasetPairDataset(
        accessor=accessor,
        decoder=decoder,
        cfg=cfg,
        device=device_used,
        task=task,
        num_styles=num_styles,
        num_contents=num_contents,
        length=train_samples,
        augment=True
    )
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        num_workers=0,  # ç¦ç”¨å¤šè¿›ç¨‹é¿å…å­˜å‚¨å†²çª
        pin_memory=True if device == "cuda" else False
    )
    
    # 4. åˆ›å»ºæ¨¡å‹
    print("\nğŸ¤– åˆ›å»ºSiameseæ¨¡å‹...")
    # ä»ä¸€ä¸ªæ ·æœ¬æ¨æ–­é€šé“æ•°
    sample_img = decode_to_image(decoder, accessor.get(0, 0), cfg, device_used)
    C = sample_img.shape[0]
    print(f"  - è¾“å…¥é€šé“æ•°: {C}")
    
    model = SiameseJudge(in_ch=C, emb_dim=512, mlp_hidden=512)  # å¢å¤§æ¨¡å‹å®¹é‡
    
    # 5. è®­ç»ƒæ¨¡å‹
    print("\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
    model = train_full_model(model, train_loader, device=device, lr=lr, epochs=epochs)
    
    # 6. è¯„ä¼°æ¨¡å‹
    print("\nğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    eval_model(model, accessor, decoder, cfg, device_used, task, num_styles, num_contents, eval_samples)
    
    return model, accessor, decoder

def train_full_model(model, loader, device="cuda", lr=1e-4, epochs=20):
    """è®­ç»ƒå®Œæ•´æ¨¡å‹"""
    model = model.to(device)
    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)
    crit = nn.BCEWithLogitsLoss()
    
    for ep in range(1, epochs+1):
        model.train()
        tot_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (x1, x2, y) in enumerate(loader):
            x1, x2, y = x1.to(device), x2.to(device), y.to(device)
            
            logit, _, _ = model(x1, x2)
            loss = crit(logit, y)
            
            opt.zero_grad()
            loss.backward()
            opt.step()
            
            # è®¡ç®—å‡†ç¡®ç‡
            pred = (torch.sigmoid(logit) > 0.5).float()
            correct += (pred == y).sum().item()
            total += y.size(0)
            tot_loss += loss.item() * x1.size(0)
            
            # æ¯100ä¸ªbatchæ‰“å°ä¸€æ¬¡è¿›åº¦
            if (batch_idx + 1) % 100 == 0:
                avg_loss = tot_loss / (batch_idx + 1) / loader.batch_size
                acc = correct / total
                print(f"  Batch {batch_idx+1}/{len(loader)}: loss={avg_loss:.4f}, acc={acc:.3f}")
        
        scheduler.step()
        
        avg_loss = tot_loss / len(loader.dataset)
        acc = correct / total
        lr_current = scheduler.get_last_lr()[0]
        print(f"[Epoch {ep}/{epochs}] loss={avg_loss:.4f}, acc={acc:.3f}, lr={lr_current:.2e}")
    
    return model

def eval_model(model, accessor, decoder, cfg, device, task, num_styles, num_contents, eval_samples):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    model.eval()
    correct = 0
    total = 0
    
    print(f"ğŸ” è¯„ä¼° {eval_samples} ä¸ªæ ·æœ¬...")
    
    with torch.no_grad():
        for _ in range(eval_samples):
            # éšæœºé€‰æ‹©æµ‹è¯•æ ·æœ¬
            i = random.randint(0, num_contents - 1)
            j = random.randint(0, num_contents - 1)
            p = random.randint(0, num_styles - 1)
            q = random.randint(0, num_styles - 1)
            
            while j == i:
                j = random.randint(0, num_contents - 1)
            while q == p:
                q = random.randint(0, num_styles - 1)
            
            try:
                # ç”Ÿæˆæµ‹è¯•å›¾åƒ
                ci_sp = decode_to_image(decoder, accessor.get(i, p), cfg, device).to(device)
                ci_sq = decode_to_image(decoder, accessor.get(i, q), cfg, device).to(device)
                cj_sp = decode_to_image(decoder, accessor.get(j, p), cfg, device).to(device)
                
                if task == "content":
                    # æµ‹è¯•æ­£æ ·æœ¬: ç›¸åŒå†…å®¹
                    pos_prob = score_pair(model, ci_sp, ci_sq, device=device)
                    # æµ‹è¯•è´Ÿæ ·æœ¬: ä¸åŒå†…å®¹
                    neg_prob = score_pair(model, ci_sp, cj_sp, device=device)
                else:  # style
                    # æµ‹è¯•æ­£æ ·æœ¬: ç›¸åŒé£æ ¼
                    pos_prob = score_pair(model, ci_sp, cj_sp, device=device)
                    # æµ‹è¯•è´Ÿæ ·æœ¬: ä¸åŒé£æ ¼
                    neg_prob = score_pair(model, ci_sp, ci_sq, device=device)
                
                # æ£€æŸ¥åˆ†ç±»æ˜¯å¦æ­£ç¡®
                if pos_prob > 0.5:
                    correct += 1
                if neg_prob < 0.5:
                    correct += 1
                total += 2
                
            except Exception as e:
                continue
    
    accuracy = correct / total if total > 0 else 0
    print(f"âœ… è¯„ä¼°å®Œæˆ: å‡†ç¡®ç‡ = {accuracy:.3f} ({correct}/{total})")
    
    return accuracy

if __name__ == "__main__":
    # é…ç½®å‚æ•°
    local_path = "/scratch/gz2199/Content-Style-Disentangled-Representation-Learning/Font-Latent-Full-PT/font_latents_v2.pt"
    
    # è¿è¡Œå†…å®¹ä»»åŠ¡è®­ç»ƒ
    print("=" * 60)
    print("ğŸ”¤ è®­ç»ƒå†…å®¹åˆ†ç¦»ä»»åŠ¡")
    print("=" * 60)
    
    content_model, accessor, decoder = run_full_training(
        task="content",
        local_path=local_path,
        num_styles=200,      # ä½¿ç”¨200ç§é£æ ¼
        num_contents=500,    # ä½¿ç”¨500ç§å†…å®¹
        train_samples=50000, # 5ä¸‡è®­ç»ƒæ ·æœ¬
        batch_size=32,       # æ‰¹é‡å¤§å°32
        epochs=15,           # 15ä¸ªepoch
        lr=1e-4,
        eval_samples=1000
    )
    
    # ä¿å­˜å†…å®¹æ¨¡å‹
    torch.save(content_model.state_dict(), "old_content_siamese_model.pth")
    print("ğŸ’¾ å†…å®¹æ¨¡å‹å·²ä¿å­˜åˆ°: old_content_siamese_model.pth")
    
    print("\n" + "=" * 60)
    print("ğŸ¨ è®­ç»ƒé£æ ¼åˆ†ç¦»ä»»åŠ¡")
    print("=" * 60)
    
    # è¿è¡Œé£æ ¼ä»»åŠ¡è®­ç»ƒ
    style_model, _, _ = run_full_training(
        task="style",
        local_path=local_path,
        num_styles=200,
        num_contents=500,
        train_samples=50000,
        batch_size=32,
        epochs=15,
        lr=5e-5,
        eval_samples=10000
    )
    
    # ä¿å­˜é£æ ¼æ¨¡å‹
    torch.save(style_model.state_dict(), "old_style_siamese_model.pth")
    print("ğŸ’¾ é£æ ¼æ¨¡å‹å·²ä¿å­˜åˆ°: old_style_siamese_model.pth")
    
    print("\nğŸ‰ å…¨æ•°æ®é›†è®­ç»ƒå®Œæˆ!")