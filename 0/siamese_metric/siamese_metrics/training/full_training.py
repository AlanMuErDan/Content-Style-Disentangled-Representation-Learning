#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Full dataset training version - ä½¿ç”¨å®Œæ•´æ•°æ®é›†è¿›è¡Œè®­ç»ƒ
"""
import os
import random
import numpy as np
import pickle
from dataclasses import dataclass
from typing import Any, Tuple, Literal, Optional
import sys
import yaml

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from huggingface_hub import hf_hub_download
import torchvision.models as models
from torchvision import transforms
from PIL import Image

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append('/scratch/gz2199/Content-Style-Disentangled-Representation-Learning/Content-Style-Disentangled-Representation-Learning')
from models import build_encoder, build_decoder

# å¯¼å…¥æœ¬åœ°çš„font_datasetæ¨¡å—ï¼ˆPTç‰ˆæœ¬ï¼‰
sys.path.append('/scratch/gz2199/Content-Style-Disentangled-Representation-Learning/0/1/0')
from font_dataset import FourWayFontPairLatentPTDataset

# å¯¼å…¥åŸæœ‰çš„ç»„ä»¶ - å¤åˆ¶è€Œä¸æ˜¯å¯¼å…¥é¿å…è·¯å¾„é—®é¢˜
import math
import json

# å¤åˆ¶å¿…è¦çš„å‡½æ•°å’Œç±»
def load_latents_from_hub(
    repo_id: str = "YuanhengLi/Font-Latent-Full-PT",
    filename: str = "font_latents_v2.pt",
    token: Optional[str] = os.getenv("HF_TOKEN"),
    map_location: str = "cpu",
    local_path: Optional[str] = None,
):
    if local_path and os.path.exists(local_path):
        print(f"[INFO] Loading from local file: {local_path}")
        obj = torch.load(local_path, map_location=map_location)
        print(f"[INFO] Loaded from local file")
        return obj
    
    try:
        pt_path = hf_hub_download(repo_id=repo_id, filename=filename, token=token)
        obj = torch.load(pt_path, map_location=map_location)
        print(f"[INFO] Loaded from {repo_id}/{filename}")
        return obj
    except Exception as e:
        print(f"[ERROR] Failed to load from Hub: {e}")
        raise

class LatentAccessor:
    def __init__(self, raw, layout: Literal["style_content","content_style"]="style_content"):
        if isinstance(raw, dict):
            if "latents" in raw:
                raw = raw["latents"]
            elif "data" in raw:
                raw = raw["data"]
        self.raw = raw
        self.layout = layout
        
        # æ¨æ–­æ•°æ®ç»„ç»‡ç»“æ„
        if isinstance(raw, torch.Tensor):
            self.total_samples = raw.shape[0]
            print(f"[INFO] æ€»æ ·æœ¬æ•°: {self.total_samples}")
            
            # å‡è®¾æ•°æ®æ˜¯æŒ‰ [character_id * num_styles + style_id] ç»„ç»‡çš„
            # éœ€è¦æ ¹æ®å®é™…æ•°æ®ç»„ç»‡ç¡®å®šè¿™äº›å‚æ•°
            # 940ä¸‡æ ·æœ¬ï¼Œå‡è®¾500ä¸ªå­—ç¬¦ï¼Œæ¯ä¸ªå­—ç¬¦çº¦18800ç§é£æ ¼
            self.num_characters = 500  # å­—ç¬¦æ•°é‡
            self.num_styles_per_char = self.total_samples // self.num_characters
            print(f"[INFO] æ¨æ–­ç»“æ„: {self.num_characters} å­—ç¬¦ Ã— {self.num_styles_per_char} é£æ ¼/å­—ç¬¦")

    def get(self, content_i: int, style_p: int) -> torch.Tensor:
        r = self.raw
        if isinstance(r, torch.Tensor):
            # è®¡ç®—ä¸€ç»´ç´¢å¼•
            # å‡è®¾æ•°æ®ç»„ç»‡ä¸º: [content_0_style_0, content_0_style_1, ..., content_1_style_0, ...]
            if content_i >= self.num_characters:
                raise IndexError(f"content_i {content_i} >= num_characters {self.num_characters}")
            if style_p >= self.num_styles_per_char:
                raise IndexError(f"style_p {style_p} >= num_styles_per_char {self.num_styles_per_char}")
                
            idx = content_i * self.num_styles_per_char + style_p
            if idx >= self.total_samples:
                raise IndexError(f"Computed index {idx} >= total_samples {self.total_samples}")
            
            return r[idx]
        else:
            raise RuntimeError("Unsupported format")

class ImprovedLatentAccessor:
    """
    ä½¿ç”¨FourWayFontPairLatentPTDatasetçš„æ”¹è¿›ç‰ˆLatentAccessor
    è‡ªåŠ¨æ¨æ–­å®é™…çš„å­—ä½“å’Œå­—ç¬¦æ•°é‡ï¼Œä¸ä¾èµ–ç¡¬ç¼–ç å‡è®¾
    """
    def __init__(self, 
                 pt_path: str, 
                 chars_path: Optional[str] = None,
                 fonts_json: Optional[str] = None,
                 device='cpu',
                 latent_shape: Tuple[int, int, int] = (4, 16, 16)):
        self.device = device
        self.pt_path = pt_path
        
        # ä¼˜å…ˆå°è¯•ä½¿ç”¨FourWayFontPairLatentPTDataset
        if chars_path and fonts_json and os.path.exists(chars_path) and os.path.exists(fonts_json):
            print("[INFO] ä½¿ç”¨FourWayFontPairLatentPTDatasetï¼ˆæ¨èæ–¹å¼ï¼‰...")
            try:
                self.dataset = FourWayFontPairLatentPTDataset(
                    pt_path=pt_path,
                    chars_path=chars_path,
                    fonts_json=fonts_json,
                    latent_shape=latent_shape,
                    pair_num=1  # åªéœ€è¦æ¨æ–­ç»“æ„ï¼Œä¸éœ€è¦å¤§é‡æ•°æ®
                )
                
                # è·å–å®é™…çš„å­—ä½“å’Œå­—ç¬¦ä¿¡æ¯
                self.fonts = self.dataset.fonts
                self.chars = self.dataset.chars
                self.common_chars = self.dataset.common_chars
                self.num_fonts = self.dataset.n
                self.num_characters = self.dataset.m
                
                # ğŸ”¥ æ·»åŠ å…¼å®¹æ€§å±æ€§
                self.num_styles_per_char = self.num_fonts  # åœ¨æ–°ç»“æ„ä¸­ï¼Œæ¯ä¸ªå­—ç¬¦æœ‰num_fontsç§é£æ ¼
                
                print(f"[ImprovedLatentAccessor] ä½¿ç”¨FourWayDatasetå‘ç°:")
                print(f"  - å­—ä½“æ•°é‡: {self.num_fonts}")
                print(f"  - å­—ç¬¦æ•°é‡: {self.num_characters}")
                print(f"  - æ¯å­—ç¬¦é£æ ¼æ•°: {self.num_styles_per_char}")
                print(f"  - æ€»å¯èƒ½ç»„åˆ: {self.num_fonts * self.num_characters}")
                print(f"  - æ•°æ®ç»„ç»‡: font_idx * {self.num_characters} + char_idx")
                
                # ä¿å­˜æ•°æ®ç»“æ„æ˜ å°„
                self.font_to_idx = {font: i for i, font in enumerate(self.fonts)}
                self.char_to_idx = {char: i for i, char in enumerate(self.chars)}
                self.fallback_mode = False
                return
            except Exception as e:
                print(f"[WARNING] FourWayFontPairLatentPTDatasetåˆå§‹åŒ–å¤±è´¥: {e}")
                print("[INFO] é™çº§åˆ°fallbackæ¨¡å¼...")
        else:
            missing_files = []
            if not chars_path:
                missing_files.append("chars_path")
            elif not os.path.exists(chars_path):
                missing_files.append(f"chars_path({chars_path})")
            if not fonts_json:
                missing_files.append("fonts_json")
            elif not os.path.exists(fonts_json):
                missing_files.append(f"fonts_json({fonts_json})")
            
            print(f"[INFO] ç¼ºå°‘æ–‡ä»¶: {', '.join(missing_files)}")
            print("[INFO] ä½¿ç”¨fallbackæ¨¡å¼ï¼Œé»˜è®¤2056å­—ä½“Ã—4574å­—ç¬¦ç»“æ„...")
        
        # Fallbackæ¨¡å¼ï¼šä½¿ç”¨2056å­—ä½“Ã—4574å­—ç¬¦çš„é»˜è®¤ç»“æ„
        self._create_from_old_accessor(pt_path, latent_shape, device)
        
    def _create_from_old_accessor(self, pt_path: str, latent_shape: Tuple[int, int, int], device: str):
        """ä½¿ç”¨æ—§çš„LatentAccessoré€»è¾‘ä½œä¸ºfallback"""
        print("[INFO] ä½¿ç”¨fallbackæ¨¡å¼ï¼Œé‡‡ç”¨2056å­—ä½“Ã—4574å­—ç¬¦ç»“æ„...")
        
        # åŠ è½½PTæ–‡ä»¶
        blob = torch.load(pt_path, map_location="cpu")
        if isinstance(blob, dict) and "latents" in blob:
            latents = blob["latents"]
        else:
            latents = blob
            
        if isinstance(latents, torch.Tensor):
            if latents.dim() == 4:  # (N, H, W, C)
                self.total_samples = latents.shape[0]
                self.latents_hwc = latents
            else:
                self.total_samples = latents.shape[0]
                self.raw_tensor = latents
                
            # ä½¿ç”¨å‡†ç¡®çš„æ•°æ®ç»“æ„: 2056å­—ä½“ Ã— 4574å­—ç¬¦
            self.num_characters = 4574  # å­—ç¬¦æ•°é‡
            self.num_fonts = 2056       # å­—ä½“æ•°é‡
            
            # éªŒè¯æ•°æ®æ€»é‡æ˜¯å¦åŒ¹é…
            expected_total = self.num_fonts * self.num_characters
            if self.total_samples != expected_total:
                print(f"[WARNING] æ•°æ®é‡ä¸åŒ¹é…!")
                print(f"  - å®é™…æ ·æœ¬: {self.total_samples}")
                print(f"  - é¢„æœŸæ ·æœ¬: {expected_total} (2056Ã—4574)")
                print(f"  - å°†æŒ‰å®é™…æ•°æ®è°ƒæ•´ç»“æ„...")
                
                # å°è¯•å…¶ä»–å¯èƒ½çš„ç»„ç»‡æ–¹å¼
                if self.total_samples % 4574 == 0:
                    self.num_fonts = self.total_samples // 4574
                    print(f"  - è°ƒæ•´ä¸º: {self.num_fonts} å­—ä½“ Ã— 4574 å­—ç¬¦")
                elif self.total_samples % 2056 == 0:
                    self.num_characters = self.total_samples // 2056
                    print(f"  - è°ƒæ•´ä¸º: 2056 å­—ä½“ Ã— {self.num_characters} å­—ç¬¦")
                else:
                    # æœ€åçš„fallbackï¼Œå°è¯•æ¥è¿‘æ­£æ–¹å½¢çš„åˆ†å¸ƒ
                    import math
                    sqrt_total = int(math.sqrt(self.total_samples))
                    for chars in [4574, 4000, 3500, sqrt_total]:
                        if self.total_samples % chars == 0:
                            self.num_characters = chars
                            self.num_fonts = self.total_samples // chars
                            break
                    print(f"  - æœ€ç»ˆè°ƒæ•´ä¸º: {self.num_fonts} å­—ä½“ Ã— {self.num_characters} å­—ç¬¦")
            
            print(f"[INFO] Fallbackç»“æ„: {self.num_fonts} å­—ä½“ Ã— {self.num_characters} å­—ç¬¦")
            print(f"[INFO] æ•°æ®ç»„ç»‡: font_idx * {self.num_characters} + char_idx")
            
            # ğŸ”¥ æ·»åŠ å…¼å®¹æ€§å±æ€§
            self.num_styles_per_char = self.num_fonts  # åœ¨æ–°ç»“æ„ä¸­ï¼Œæ¯ä¸ªå­—ç¬¦æœ‰num_fontsç§é£æ ¼
            
            # åˆ›å»ºå‡çš„å­—ä½“å’Œå­—ç¬¦åˆ—è¡¨
            self.fonts = [f"font_{i:04d}" for i in range(self.num_fonts)]
            self.chars = [f"char_{i:04d}" for i in range(self.num_characters)]
            self.common_chars = self.chars
            
            self.font_to_idx = {font: i for i, font in enumerate(self.fonts)}
            self.char_to_idx = {char: i for i, char in enumerate(self.chars)}
            
            # åˆ›å»ºfallbackæ•°æ®é›†æ¥å£
            self.dataset = None
            self.fallback_mode = True
        else:
            raise RuntimeError("æ— æ³•è¯†åˆ«çš„æ•°æ®æ ¼å¼")
    
    def get_by_indices(self, font_idx: int, char_idx: int) -> torch.Tensor:
        """é€šè¿‡ç´¢å¼•è·å–æ½œåœ¨ç¼–ç """
        if font_idx >= self.num_fonts:
            raise IndexError(f"font_idx {font_idx} >= num_fonts {self.num_fonts}")
        if char_idx >= self.num_characters:
            raise IndexError(f"char_idx {char_idx} >= num_characters {self.num_characters}")
            
        if hasattr(self, 'fallback_mode') and self.fallback_mode:
            # ä½¿ç”¨fallbackæ¨¡å¼
            idx = font_idx * self.num_characters + char_idx
            if hasattr(self, 'latents_hwc'):
                # 4Dæ•°æ® (N, H, W, C) -> (C, H, W)
                z_hwc = self.latents_hwc[idx]
                latent = z_hwc.permute(2, 0, 1).contiguous()
            else:
                # ä½¿ç”¨åŸå§‹tensor
                latent = self.raw_tensor[idx]
            return latent.to(self.device)
        else:
            # ä½¿ç”¨æ•°æ®é›†çš„å†…éƒ¨æ–¹æ³•è·å–æ½œåœ¨ç¼–ç 
            flat_idx = self.dataset._flat_index(font_idx, char_idx)
            latent = self.dataset._get_chw(flat_idx)
            return latent.to(self.device)
    
    def get_by_names(self, font_name: str, char_name: str) -> torch.Tensor:
        """é€šè¿‡åç§°è·å–æ½œåœ¨ç¼–ç """
        if font_name not in self.font_to_idx:
            raise KeyError(f"Font '{font_name}' not found")
        if char_name not in self.char_to_idx:
            raise KeyError(f"Char '{char_name}' not found")
            
        font_idx = self.font_to_idx[font_name]
        char_idx = self.char_to_idx[char_name]
        
        return self.get_by_indices(font_idx, char_idx)
    
    def get(self, content_i: int, style_p: int) -> torch.Tensor:
        """å…¼å®¹åŸæ¥å£çš„æ–¹æ³•"""
        # content_iå¯¹åº”å­—ç¬¦ç´¢å¼•ï¼Œstyle_på¯¹åº”å­—ä½“ç´¢å¼•
        return self.get_by_indices(style_p % self.num_fonts, content_i % self.num_characters)

# VAEè§£ç å™¨ç›¸å…³å‡½æ•° - ä½¿ç”¨vae_io.pyçš„å®ç°
def _load_config(path: str) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(path, "r") as f:
        cfg = yaml.safe_load(f)
    return cfg["vae"] if "vae" in cfg else cfg

def load_vae_decoder(
    config_path: str = "/scratch/gz2199/Content-Style-Disentangled-Representation-Learning/Content-Style-Disentangled-Representation-Learning/configs/config.yaml",
    ckpt_path: str = "/scratch/gz2199/Content-Style-Disentangled-Representation-Learning/0/1/0/vae_best_ckpt.pth",
    device: str = "cuda"
) -> Tuple[torch.nn.Module, dict, torch.device]:
    """
    åŠ è½½VAEè§£ç å™¨
    
    Returns:
        decoder  â€“ torch.nn.Module (eval mode)
        cfg      â€“ dict (é…ç½®)
        device   â€“ torch.device
    """
    device = torch.device(device if torch.cuda.is_available() else "cpu")
    cfg = _load_config(config_path)

    decoder = build_decoder(
        name=cfg["decoder"],
        img_size=cfg["img_size"],
        latent_channels=cfg.get("latent_channels", 4),
    ).to(device).eval()

    # åŠ è½½æƒé‡
    ckpt = torch.load(ckpt_path, map_location=device)
    if "decoder" in ckpt:
        decoder.load_state_dict(ckpt["decoder"])
    else:
        # å¦‚æœcheckpointç›´æ¥æ˜¯decoderçŠ¶æ€
        decoder.load_state_dict(ckpt)

    print(f"[INFO] Loaded VAE decoder on {device}")
    return decoder, cfg, device

@torch.no_grad()
def decode_to_image(decoder: nn.Module, z: torch.Tensor, cfg: dict, device: torch.device) -> torch.Tensor:
    """
    ä½¿ç”¨VAEè§£ç å™¨å°†æ½œåœ¨ç¼–ç è§£ç ä¸ºå›¾åƒ
    
    Args:
        decoder: VAEè§£ç å™¨
        z: æ½œåœ¨ç¼–ç  [C,H,W] æˆ–å…¶ä»–æ ¼å¼
        cfg: é…ç½®å­—å…¸
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        img: å›¾åƒå¼ é‡ [C,H,W] èŒƒå›´[0,1]
    """
    # ğŸ”¥ æ£€æŸ¥å¹¶ä¿®å¤æ•°æ®ç±»å‹ä¸åŒ¹é…é—®é¢˜ï¼ˆå‡å°‘æ—¥å¿—è¾“å‡ºï¼‰
    decoder_dtype = next(decoder.parameters()).dtype
    if z.dtype != decoder_dtype:
        # åªåœ¨ç¬¬ä¸€æ¬¡è½¬æ¢æ—¶è¾“å‡ºä¿¡æ¯ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
        if not hasattr(decode_to_image, '_conversion_logged'):
            print(f"[INFO] è‡ªåŠ¨è½¬æ¢æ½œåœ¨ç¼–ç ç±»å‹: {z.dtype} -> {decoder_dtype}")
            decode_to_image._conversion_logged = True
        z = z.to(dtype=decoder_dtype)
    
    # ç¡®ä¿æ½œåœ¨ç¼–ç æ ¼å¼æ­£ç¡®
    if z.dim() == 2:
        # å¦‚æœæ˜¯2Dï¼Œå°è¯•é‡å¡‘ä¸º3D
        if z.shape == (16, 4):
            z = z.view(4, 4, 4)  # [C, H, W]
        else:
            raise ValueError(f"Unexpected 2D tensor shape: {z.shape}")
    elif z.dim() == 1:
        # å¦‚æœæ˜¯1Dï¼Œé‡å¡‘ä¸º3D
        if z.numel() == 64:  # 4*4*4
            z = z.view(4, 4, 4)
        else:
            raise ValueError(f"Unexpected 1D tensor size: {z.numel()}")
    elif z.dim() == 3:
        # ğŸ”¥ 3Då¼ é‡ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°æ’åˆ—ç»´åº¦
        if z.shape == (16, 16, 4):
            z = z.permute(2, 0, 1)  # [H, W, C] -> [C, H, W]
            if not hasattr(decode_to_image, '_reshape_logged'):
                print(f"[INFO] é‡æ–°æ’åˆ—ç»´åº¦: (16,16,4) -> {z.shape}")
                decode_to_image._reshape_logged = True
    elif z.dim() != 3:
        raise ValueError(f"Latent must be 1D, 2D, or 3D, got {z.dim()}D")
    
    # æ·»åŠ batchç»´åº¦å¹¶ç§»åˆ°è®¾å¤‡ï¼Œç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
    z_batch = z.unsqueeze(0).to(device=device, dtype=decoder_dtype)  # [1, C, H, W]
    
    # è§£ç 
    with torch.no_grad():
        recon = decoder(z_batch).squeeze(0).cpu()  # [C, H, W]
    
    # ç¡®ä¿è¾“å‡ºåœ¨[0,1]èŒƒå›´
    recon = torch.clamp(recon, 0, 1)
    
    return recon
class VGGEncoder(nn.Module):
    """åŸºäºVGGçš„ç¼–ç å™¨ - ä½¿ç”¨é¢„è®­ç»ƒç‰¹å¾"""
    def __init__(self, in_ch=1, emb_dim=512, vgg_variant="vgg16", task="content"):
        super().__init__()
        self.task = task
        
        # åŠ è½½é¢„è®­ç»ƒVGGæ¨¡å‹
        if vgg_variant == "vgg16":
            vgg = models.vgg16(pretrained=True)
        elif vgg_variant == "vgg19":
            vgg = models.vgg19(pretrained=True)
        else:
            raise ValueError(f"Unsupported VGG variant: {vgg_variant}")
        
        # æå–VGGç‰¹å¾å±‚
        self.features = vgg.features
        
        # è¾“å…¥é€šé“é€‚é… (ç°åº¦å›¾ â†’ RGB)
        if in_ch == 1:
            # å°†ç¬¬ä¸€å±‚Conv2dä»3é€šé“æ”¹ä¸º1é€šé“
            first_conv = self.features[0]
            self.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)
            # å¤åˆ¶é¢„è®­ç»ƒæƒé‡çš„å‡å€¼åˆ°å•é€šé“
            with torch.no_grad():
                self.features[0].weight = nn.Parameter(
                    first_conv.weight.mean(dim=1, keepdim=True)
                )
                self.features[0].bias = first_conv.bias.clone()
        
        # å†»ç»“ä½å±‚ç‰¹å¾ (å¯é€‰)
        self._freeze_early_layers(freeze_layers=3)
        
        # æ ¹æ®ä»»åŠ¡é€‰æ‹©ä¸åŒçš„ç‰¹å¾æå–ç­–ç•¥
        if task == "content":
            # å†…å®¹ä»»åŠ¡ï¼šä½¿ç”¨ä¸­å±‚ç‰¹å¾ï¼Œä¿ç•™ç©ºé—´ä¿¡æ¯
            self.feature_layers = [10, 17, 24]  # conv2_2, conv3_4, conv4_4
        else:  # style
            # é£æ ¼ä»»åŠ¡ï¼šä½¿ç”¨å¤šå±‚ç‰¹å¾ï¼Œæå–çº¹ç†ä¿¡æ¯
            self.feature_layers = [3, 8, 15, 22, 29]  # conv1_2, conv2_2, conv3_3, conv4_3, conv5_3
        
        # è‡ªé€‚åº”æ± åŒ–å±‚
        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4)) if task == "content" else nn.AdaptiveAvgPool2d((1, 1))
        
        # ç‰¹å¾èåˆå±‚
        self.feature_dim = self._calculate_feature_dim()
        self.fc = nn.Sequential(
            nn.Linear(self.feature_dim, emb_dim * 2),
            nn.BatchNorm1d(emb_dim * 2),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(emb_dim * 2, emb_dim),
            nn.BatchNorm1d(emb_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2)
        )
        
    def _freeze_early_layers(self, freeze_layers=3):
        """å†»ç»“å‰å‡ å±‚çš„å‚æ•°"""
        for i, module in enumerate(self.features.children()):
            if i < freeze_layers:
                for param in module.parameters():
                    param.requires_grad = False
    
    def _calculate_feature_dim(self):
        """è®¡ç®—ç‰¹å¾ç»´åº¦"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„VGGç»“æ„å’Œé€‰æ‹©çš„å±‚æ¥è®¡ç®—
        # ç®€åŒ–è®¡ç®—ï¼Œå‡è®¾æ¯å±‚512ç»´ç‰¹å¾
        if self.task == "content":
            return 512 * 16  # 4x4 spatial
        else:
            return 512 * len(self.feature_layers)  # å¤šå±‚ç‰¹å¾æ‹¼æ¥
    
    def forward(self, x):
        if x.ndim == 3:
            x = x.unsqueeze(0)
        if x.size(1) not in (1, 3):
            x = x.mean(dim=1, keepdim=True)
        
        # å¦‚æœè¾“å…¥æ˜¯å•é€šé“ä½†ç½‘ç»œæœŸæœ›ä¸‰é€šé“
        if x.size(1) == 1 and self.features[0].in_channels == 3:
            x = x.repeat(1, 3, 1, 1)
        
        features = []
        for i, layer in enumerate(self.features):
            x = layer(x)
            if i in self.feature_layers:
                # åº”ç”¨è‡ªé€‚åº”æ± åŒ–
                pooled = self.adaptive_pool(x)
                features.append(pooled.view(pooled.size(0), -1))
        
        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        if features:
            combined_features = torch.cat(features, dim=1)
        else:
            # å¦‚æœæ²¡æœ‰æå–åˆ°ç‰¹å¾ï¼Œä½¿ç”¨æœ€åçš„è¾“å‡º
            combined_features = self.adaptive_pool(x).view(x.size(0), -1)
        
        return self.fc(combined_features)

class HybridEncoder(nn.Module):
    """æ··åˆç¼–ç å™¨ - ç»“åˆVGGå’Œè‡ªå®šä¹‰CNN"""
    def __init__(self, in_ch=1, emb_dim=512, task="content"):
        super().__init__()
        self.task = task
        
        # VGGåˆ†æ”¯
        self.vgg_branch = VGGEncoder(in_ch=in_ch, emb_dim=emb_dim//2, task=task)
        
        # è‡ªå®šä¹‰CNNåˆ†æ”¯
        if task == "content":
            self.custom_branch = ContentEncoder(in_ch=in_ch, emb_dim=emb_dim//2)
        else:
            self.custom_branch = StyleEncoder(in_ch=in_ch, emb_dim=emb_dim//2)
        
        # ç‰¹å¾èåˆ
        self.fusion = nn.Sequential(
            nn.Linear(emb_dim, emb_dim),
            nn.BatchNorm1d(emb_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2)
        )
        
    def forward(self, x):
        vgg_features = self.vgg_branch(x)
        custom_features = self.custom_branch(x)
        
        # æ‹¼æ¥ç‰¹å¾
        combined = torch.cat([vgg_features, custom_features], dim=1)
        
        return self.fusion(combined)

class EnhancedContentEncoder(nn.Module):
    """å¢å¼ºçš„å†…å®¹ç¼–ç å™¨ - æ›´æ·±çš„ç½‘ç»œ"""
    def __init__(self, in_ch=1, emb_dim=512):
        super().__init__()
        
        # æ›´æ·±çš„å·ç§¯ç½‘ç»œ - å—VGGå¯å‘çš„è®¾è®¡
        self.features = nn.Sequential(
            # Block 1 - 64 channels
            nn.Conv2d(in_ch, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            
            # Block 2 - 128 channels  
            nn.Conv2d(64, 128, 3, 2, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),  # 32â†’16
            nn.Conv2d(128, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            
            # Block 3 - 256 channels
            nn.Conv2d(128, 256, 3, 2, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),  # 16â†’8
            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            
            # Block 4 - 512 channels
            nn.Conv2d(256, 512, 3, 2, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),  # 8â†’4
            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            
            # Block 5 - æ·±å±‚ç‰¹å¾
            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Dropout(0.2)
        )
        
        self.spatial_pool = nn.AdaptiveAvgPool2d(4)  # ä¿ç•™4x4ç©ºé—´ä¿¡æ¯
        
        self.classifier = nn.Sequential(
            nn.Linear(512 * 16, emb_dim * 2),
            nn.BatchNorm1d(emb_dim * 2),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(emb_dim * 2, emb_dim),
            nn.BatchNorm1d(emb_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2)
        )

    def forward(self, x):
        if x.ndim == 3:
            x = x.unsqueeze(0)
        if x.size(1) not in (1,3):
            x = x.mean(dim=1, keepdim=True)
        
        h = self.features(x)
        h = self.spatial_pool(h)
        h = h.view(h.size(0), -1)
        return self.classifier(h)

class EnhancedStyleEncoder(nn.Module):
    """å¢å¼ºçš„é£æ ¼ç¼–ç å™¨ - VGGé£æ ¼çš„å¤šå±‚ç‰¹å¾"""
    def __init__(self, in_ch=1, emb_dim=512):
        super().__init__()
        
        # VGGé£æ ¼çš„ç‰¹å¾æå–å™¨
        self.features = nn.Sequential(
            # Block 1
            nn.Conv2d(in_ch, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 32â†’16
            
            # Block 2  
            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 16â†’8
            
            # Block 3
            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 8â†’4
            
            # Block 4
            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            
            # Block 5 - æ·±å±‚çº¹ç†ç‰¹å¾
            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Dropout(0.3)
        )
        
        # å¤šå°ºåº¦æ± åŒ–
        self.global_pools = nn.ModuleList([
            nn.AdaptiveAvgPool2d(1),    # å…¨å±€å¹³å‡
            nn.AdaptiveMaxPool2d(1),    # å…¨å±€æœ€å¤§
            nn.AdaptiveAvgPool2d(2),    # 2x2å¹³å‡
        ])
        
        self.classifier = nn.Sequential(
            nn.Linear(512 * 6, emb_dim * 2),  # 512*1 + 512*1 + 512*4 = 512*6
            nn.BatchNorm1d(emb_dim * 2),
            nn.ReLU(inplace=True),
            nn.Dropout(0.4),
            nn.Linear(emb_dim * 2, emb_dim),
            nn.BatchNorm1d(emb_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3)
        )

    def forward(self, x):
        if x.ndim == 3:
            x = x.unsqueeze(0)
        if x.size(1) not in (1,3):
            x = x.mean(dim=1, keepdim=True)
        
        h = self.features(x)
        
        # å¤šå°ºåº¦ç‰¹å¾æå–
        pooled_features = []
        for pool in self.global_pools:
            pooled = pool(h).view(h.size(0), -1)
            pooled_features.append(pooled)
        
        h = torch.cat(pooled_features, dim=1)
        return self.classifier(h)
    """å†…å®¹ç¼–ç å™¨ - ä¿ç•™æ›´å¤šç©ºé—´ä¿¡æ¯ï¼Œå°‘ç”¨æ± åŒ–"""
    def __init__(self, in_ch=1, emb_dim=512):
        super().__init__()
        # ğŸ”¥ æ›´å°‘æ± åŒ–ï¼Œä¿ç•™ç©ºé—´ç»†èŠ‚ç”¨äºå†…å®¹è¯†åˆ«
        self.spatial_net = nn.Sequential(
            # ç¬¬ä¸€å±‚ï¼šä¿æŒåˆ†è¾¨ç‡ï¼Œæå–ç»†ç²’åº¦ç‰¹å¾
            nn.Conv2d(in_ch, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),  # é¢å¤–å±‚
            
            # ç¬¬äºŒå±‚ï¼šè½»å¾®ä¸‹é‡‡æ ·ï¼Œä¿ç•™ç»“æ„ä¿¡æ¯
            nn.Conv2d(64, 128, 3, 2, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),  # é¢å¤–å±‚
            
            # ç¬¬ä¸‰å±‚ï¼šæå–å±€éƒ¨å†…å®¹ç‰¹å¾
            nn.Conv2d(128, 256, 3, 2, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            
            # ç¬¬å››å±‚ï¼šé«˜çº§ç‰¹å¾ä½†ä¿ç•™æ›´å¤šç©ºé—´ä¿¡æ¯
            nn.Conv2d(256, 512, 3, 2, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Dropout(0.2)
        )
        
        # ğŸ”¥ ä¿ç•™æ›´å¤šç©ºé—´ä¿¡æ¯ - 4x4è€Œä¸æ˜¯2x2
        self.spatial_pool = nn.AdaptiveAvgPool2d(4)  # ä¿ç•™4x4ç©ºé—´ä¿¡æ¯
        
        self.fc = nn.Sequential(
            nn.Linear(512 * 16, emb_dim),  # 4x4=16
            nn.BatchNorm1d(emb_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3)
        )

    def forward(self, x):
        if x.ndim == 3:
            x = x.unsqueeze(0)
        if x.size(1) not in (1,3):
            x = x.mean(dim=1, keepdim=True)
        
        h = self.spatial_net(x)
        h = self.spatial_pool(h)  # [B, 512, 4, 4]
        h = h.view(h.size(0), -1)  # [B, 512*16] = [B, 8192]
        return self.fc(h)

class StyleEncoder(nn.Module):
    """é£æ ¼ç¼–ç å™¨ - æ›´å¤šæ± åŒ–ï¼Œæå–æŠ½è±¡ç‰¹å¾"""
    def __init__(self, in_ch=1, emb_dim=512):
        super().__init__()
        # ä¿®å¤æ± åŒ–é—®é¢˜ï¼šå‡å°‘ä¸‹é‡‡æ ·å±‚æ•°ï¼Œç¡®ä¿ä¸ä¼šå‹ç¼©åˆ°0
        self.abstract_net = nn.Sequential(
            # ç¬¬ä¸€å±‚ï¼šåŸºç¡€ç‰¹å¾æå–
            nn.Conv2d(in_ch, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),  # 32â†’32
            nn.Conv2d(64, 64, 3, 2, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),  # 32â†’16
            
            # ç¬¬äºŒå±‚ï¼šçº¹ç†ç‰¹å¾
            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),  # 16â†’16
            nn.Conv2d(128, 128, 3, 2, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),  # 16â†’8
            
            # ç¬¬ä¸‰å±‚ï¼šé£æ ¼ç‰¹å¾
            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),  # 8â†’8
            nn.Conv2d(256, 256, 3, 2, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),  # 8â†’4
            
            # ç¬¬å››å±‚ï¼šé«˜çº§é£æ ¼ç‰¹å¾
            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),  # 4â†’4
            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),  # 4â†’4
            nn.Dropout(0.3)
        )
        
        # ç¡®ä¿æ± åŒ–åä¸ä¼šå˜æˆ0å°ºå¯¸
        self.global_pools = nn.ModuleList([
            nn.AdaptiveAvgPool2d(1),     # å…¨å±€å¹³å‡æ± åŒ–åˆ°1x1
            nn.AdaptiveMaxPool2d(1),     # å…¨å±€æœ€å¤§æ± åŒ–åˆ°1x1
        ])
        
        self.fc = nn.Sequential(
            nn.Linear(1024, emb_dim),  # ä¸¤ç§æ± åŒ–ç‰¹å¾æ‹¼æ¥: 512*1 + 512*1 = 1024
            nn.BatchNorm1d(emb_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.4)
        )

    def forward(self, x):
        if x.ndim == 3:
            x = x.unsqueeze(0)
        if x.size(1) not in (1,3):
            x = x.mean(dim=1, keepdim=True)
        
        h = self.abstract_net(x)
        
        # ğŸ”¥ ä¿®å¤åçš„æ± åŒ–ç‰¹å¾ç»„åˆ - åªä½¿ç”¨ä¸¤ç§æ± åŒ–æ–¹å¼
        pooled_features = []
        for pool in self.global_pools:
            pooled = pool(h).view(h.size(0), -1)
            pooled_features.append(pooled)
        
        h = torch.cat(pooled_features, dim=1)  # [B, 512*2]
        return self.fc(h)

class TinyEncoder(nn.Module):
    """ä¸´æ—¶ä½¿ç”¨åŸæ¥çš„ç¼–ç å™¨ç¡®ä¿å…¼å®¹æ€§"""
    def __init__(self, in_ch=1, emb_dim=512, task="content"):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(in_ch, 64, 3, 2, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, 3, 2, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, 3, 2, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.Conv2d(256, 512, 3, 2, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, 2, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d(1),
            nn.Dropout(0.3)
        )
        self.fc = nn.Sequential(
            nn.Linear(512, emb_dim),
            nn.BatchNorm1d(emb_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3)
        )

    def forward(self, x):
        if x.ndim == 3:
            x = x.unsqueeze(0)
        if x.size(1) not in (1,3):
            x = x.mean(dim=1, keepdim=True)
        h = self.net(x)
        h = h.view(h.size(0), -1)
        return self.fc(h)

class SiameseJudge(nn.Module):
    def __init__(self, in_ch=1, emb_dim=512, mlp_hidden=512, task="content", encoder_type="enhanced"):
        super().__init__()
        
        # ğŸ”¥ æ”¯æŒå¤šç§ç¼–ç å™¨ç±»å‹
        if encoder_type == "enhanced":
            # å¢å¼ºç‰ˆç¼–ç å™¨ - VGGé£æ ¼çš„æ·±å±‚ç½‘ç»œ
            if task == "content":
                self.encoder = EnhancedContentEncoder(in_ch=in_ch, emb_dim=emb_dim)
            else:  # style
                self.encoder = EnhancedStyleEncoder(in_ch=in_ch, emb_dim=emb_dim)
        elif encoder_type == "vgg":
            # çº¯VGGç¼–ç å™¨
            self.encoder = VGGEncoder(in_ch=in_ch, emb_dim=emb_dim, task=task)
        elif encoder_type == "hybrid":
            # æ··åˆç¼–ç å™¨ - VGG + è‡ªå®šä¹‰CNN
            self.encoder = HybridEncoder(in_ch=in_ch, emb_dim=emb_dim, task=task)
        elif encoder_type == "original":
            # åŸå§‹ç¼–ç å™¨
            if task == "content":
                self.encoder = ContentEncoder(in_ch=in_ch, emb_dim=emb_dim)
            else:  # style
                self.encoder = StyleEncoder(in_ch=in_ch, emb_dim=emb_dim)
        else:
            raise ValueError(f"Unsupported encoder_type: {encoder_type}")
            
        # æ›´æ·±çš„åˆ†ç±»å¤´
        self.head = nn.Sequential(
            nn.Linear(emb_dim, mlp_hidden), 
            nn.BatchNorm1d(mlp_hidden),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(mlp_hidden, mlp_hidden//2),
            nn.BatchNorm1d(mlp_hidden//2),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(mlp_hidden//2, mlp_hidden//4),
            nn.BatchNorm1d(mlp_hidden//4),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(mlp_hidden//4, 1)
        )

    def forward(self, x1, x2):
        v1 = self.encoder(x1)
        v2 = self.encoder(x2)
        diff = torch.abs(v1 - v2)
        logit = self.head(diff)
        return logit.squeeze()  # ğŸ”¥ åªè¿”å›logitï¼Œå¹¶ç¡®ä¿æ­£ç¡®çš„å½¢çŠ¶

@torch.no_grad()
def score_pair(model: nn.Module, img1: torch.Tensor, img2: torch.Tensor, device=None) -> float:
    device = device or next(model.parameters()).device
    model.eval()
    img1 = img1.unsqueeze(0).to(device)
    img2 = img2.unsqueeze(0).to(device)
    logit = model(img1, img2)  # ğŸ”¥ ç°åœ¨åªè¿”å›logit
    return torch.sigmoid(logit).item()

class FullDatasetPairDataset(Dataset):
    """
    ä½¿ç”¨å®Œæ•´æ•°æ®é›†çš„é…å¯¹æ•°æ®é›†
    ä»å¤§é‡æ ·æœ¬ä¸­éšæœºé€‰æ‹©é…å¯¹è¿›è¡Œè®­ç»ƒ
    """
    def __init__(self, 
                 accessor: ImprovedLatentAccessor, 
                 decoder: nn.Module,
                 cfg: dict,
                 device_used: torch.device,
                 task: Literal["content","style"]="content",
                 num_styles: int = 100,
                 num_contents: int = 1000,
                 length: int = 50000,  # å¢åŠ è®­ç»ƒæ ·æœ¬æ•°
                 augment: bool = True,
                 device: str = "cpu"):
        """
        Args:
            accessor: æ½œåœ¨ç¼–ç è®¿é—®å™¨
            decoder: VAEè§£ç å™¨æ¨¡å‹
            cfg: VAEé…ç½®å­—å…¸
            device_used: è§£ç å™¨æ‰€åœ¨è®¾å¤‡
            task: ä»»åŠ¡ç±»å‹ ('content' æˆ– 'style')
            num_styles: ä½¿ç”¨çš„é£æ ¼æ•°é‡
            num_contents: ä½¿ç”¨çš„å†…å®¹æ•°é‡  
            length: è®­ç»ƒæ ·æœ¬æ€»æ•°
            augment: æ˜¯å¦ä½¿ç”¨æ•°æ®å¢å¼º
            device: è®¡ç®—è®¾å¤‡
        """
        self.accessor = accessor
        self.decoder = decoder
        self.cfg = cfg
        self.device_used = device_used
        self.task = task
        self.num_styles = num_styles
        self.num_contents = num_contents
        self.length = length
        self.augment = augment
        self.device = device
        
        print(f"[INFO] åˆ›å»ºå…¨æ•°æ®é›†è®­ç»ƒå™¨:")
        print(f"  - ä»»åŠ¡ç±»å‹: {task}")
        print(f"  - é£æ ¼æ•°é‡: {num_styles}")
        print(f"  - å†…å®¹æ•°é‡: {num_contents}")
        print(f"  - è®­ç»ƒæ ·æœ¬: {length}")
        
        # é¢„å…ˆæ£€æŸ¥æ•°æ®è®¿é—®æ˜¯å¦æ­£å¸¸
        try:
            test_sample = self.accessor.get(0, 0)
            print(f"  - æ ·æœ¬å½¢çŠ¶: {test_sample.shape}")
        except Exception as e:
            print(f"  - è­¦å‘Š: æ•°æ®è®¿é—®æµ‹è¯•å¤±è´¥: {e}")

    def __len__(self):
        return self.length

    def _aug(self, x: torch.Tensor) -> torch.Tensor:
        """æ•°æ®å¢å¼º"""
        if not self.augment: 
            return x
        # ç®€å•çš„å‡ ä½•å’Œå™ªå£°å¢å¼º
        if random.random() < 0.5:
            x = torch.flip(x, dims=[2])  # æ°´å¹³ç¿»è½¬
        if random.random() < 0.2:
            x = x + torch.randn_like(x) * 0.02
            x = torch.clamp(x, 0.0, 1.0)
        return x

    def __getitem__(self, idx):
        """è·å–è®­ç»ƒæ ·æœ¬å¯¹ - æ”¹è¿›çš„é‡‡æ ·ç­–ç•¥"""
        # è·å–å®é™…å¯ç”¨çš„èŒƒå›´
        max_contents = min(self.num_contents, self.accessor.num_characters)
        max_styles = min(self.num_styles, self.accessor.num_styles_per_char)
        
        # éšæœºé€‰æ‹©ç´¢å¼• - ä½¿ç”¨å®‰å…¨çš„èŒƒå›´
        content_i = random.randint(0, max_contents - 1)
        content_j = random.randint(0, max_contents - 1)
        style_p = random.randint(0, max_styles - 1)
        style_q = random.randint(0, max_styles - 1)
        
        # ç¡®ä¿æœ‰ä¸åŒçš„é€‰æ‹©ç”¨äºè´Ÿæ ·æœ¬
        while content_j == content_i:
            content_j = random.randint(0, max_contents - 1)
        while style_q == style_p:
            style_q = random.randint(0, max_styles - 1)
        
        # æ”¹è¿›çš„é‡‡æ ·ç­–ç•¥ï¼šé’ˆå¯¹æ€§å¢åŠ å›°éš¾æ ·æœ¬çš„æ¦‚ç‡
        if self.task == "content":
            # Contentä»»åŠ¡ï¼š80%æ¦‚ç‡ç”Ÿæˆè´Ÿæ ·æœ¬ï¼ˆä¸åŒå†…å®¹ç›¸åŒé£æ ¼ - æœ€å›°éš¾ï¼‰
            is_positive = (random.random() < 0.2)
        else:  # style task
            # Styleä»»åŠ¡ï¼š75%æ¦‚ç‡ç”Ÿæˆæ­£æ ·æœ¬ï¼ˆç›¸åŒé£æ ¼ä¸åŒå†…å®¹ - å›°éš¾æ ·æœ¬ï¼‰
            is_positive = (random.random() < 0.75)
        
        try:
            # è·å–æ½œåœ¨ç¼–ç 
            z1 = self.accessor.get(content_i, style_p)
            if idx < 5:  # åªåœ¨å‰å‡ ä¸ªæ ·æœ¬æ‰“å°è°ƒè¯•ä¿¡æ¯
                print(f"[DEBUG] Sample {idx}: z1.shape = {z1.shape}")
            
            # è§£ç ä¸ºå›¾åƒ (ä½¿ç”¨VAEè§£ç å™¨)
            ci_sp = decode_to_image(self.decoder, z1, self.cfg, self.device_used)
            if idx < 5:
                print(f"[DEBUG] Sample {idx}: ci_sp.shape = {ci_sp.shape}")
            
            if self.task == "content":
                if is_positive:
                    # æ­£æ ·æœ¬: ç›¸åŒå†…å®¹ï¼Œä¸åŒé£æ ¼
                    z2 = self.accessor.get(content_i, style_q)
                    ci_sq = decode_to_image(self.decoder, z2, self.cfg, self.device_used)
                    x1, x2, y = ci_sp, ci_sq, 1.0
                else:
                    # è´Ÿæ ·æœ¬: ä¸åŒå†…å®¹ï¼Œç›¸åŒé£æ ¼ (å›°éš¾æ ·æœ¬)
                    # ğŸ”¥ ç­–ç•¥æ”¹è¿›ï¼šé€‰æ‹©ç›¸ä¼¼çš„å†…å®¹å¢åŠ éš¾åº¦
                    if random.random() < 0.3:  # 30%æ¦‚ç‡é€‰æ‹©ç›¸é‚»å†…å®¹
                        content_j = min(max_contents - 1, content_i + 1)
                    z2 = self.accessor.get(content_j, style_p)
                    cj_sp = decode_to_image(self.decoder, z2, self.cfg, self.device_used)
                    x1, x2, y = ci_sp, cj_sp, 0.0
            else:  # style task
                if is_positive:
                    # æ­£æ ·æœ¬: ç›¸åŒé£æ ¼ï¼Œä¸åŒå†…å®¹ (å›°éš¾æ ·æœ¬)
                    # ğŸ”¥ ç­–ç•¥æ”¹è¿›ï¼šé€‰æ‹©å·®å¼‚è¾ƒå¤§çš„å†…å®¹
                    if random.random() < 0.3:  # 30%æ¦‚ç‡é€‰æ‹©å·®å¼‚å¤§çš„å†…å®¹
                        content_j = (content_i + max_contents // 2) % max_contents
                    z2 = self.accessor.get(content_j, style_p)
                    cj_sp = decode_to_image(self.decoder, z2, self.cfg, self.device_used)
                    x1, x2, y = ci_sp, cj_sp, 1.0
                else:
                    # è´Ÿæ ·æœ¬: ä¸åŒé£æ ¼ï¼Œç›¸åŒå†…å®¹
                    z2 = self.accessor.get(content_i, style_q)
                    ci_sq = decode_to_image(self.decoder, z2, self.cfg, self.device_used)
                    x1, x2, y = ci_sp, ci_sq, 0.0

            # åº”ç”¨æ•°æ®å¢å¼º
            x1 = self._aug(x1)
            x2 = self._aug(x2)
            
            # ç¡®ä¿å¼ é‡æ˜¯è¿ç»­çš„å¹¶ä¸”å¯ä»¥è¢«å¤åˆ¶
            x1 = x1.contiguous().detach().clone()
            x2 = x2.contiguous().detach().clone()
            
            return x1, x2, torch.tensor(y, dtype=torch.float32)  # ğŸ”¥ è¿”å›æ ‡é‡è€Œä¸æ˜¯[y]
            
        except Exception as e:
            print(f"[WARN] æ ·æœ¬ç”Ÿæˆå¤±è´¥ (idx={idx}): {e}")
            # è¿”å›ä¸€ä¸ªdummyæ ·æœ¬é¿å…è®­ç»ƒä¸­æ–­
            dummy_img = torch.zeros(1, 32, 32)
            return dummy_img, dummy_img, torch.tensor(0.0, dtype=torch.float32)  # ğŸ”¥ è¿”å›æ ‡é‡

def run_full_training(
    task: Literal["content","style"]="content",
    layout: Literal["style_content","content_style"]="style_content",
    
    # PTæ•°æ®è·¯å¾„å‚æ•°
    pt_path: str = "/scratch/gz2199/Content-Style-Disentangled-Representation-Learning/Font-Latent-Full-PT/font_latents_v2.pt",
    chars_path: Optional[str] = "/scratch/gz2199/Content-Style-Disentangled-Representation-Learning/0/1/0/intersection_chars_full.txt",  # ğŸ”¥ åœ¨è¿™é‡Œè®¾ç½®ä½ çš„å­—ç¬¦æ–‡ä»¶è·¯å¾„
    fonts_json: Optional[str] = "/scratch/gz2199/Content-Style-Disentangled-Representation-Learning/0/1/0/font_list(1).json",  # ğŸ”¥ åœ¨è¿™é‡Œè®¾ç½®ä½ çš„å­—ä½“æ–‡ä»¶è·¯å¾„
    
    decoder_name: str = "diff_decoder",
    device="cuda" if torch.cuda.is_available() else "cpu",
    encoder_type: str = "enhanced",  # ğŸ”¥ æ–°å¢ç¼–ç å™¨ç±»å‹é€‰æ‹©
    
    # è®­ç»ƒå‚æ•°
    num_styles: int = 500,      # ä½¿ç”¨çš„é£æ ¼æ•°é‡ (å‡å°‘åˆ°åˆç†èŒƒå›´)
    num_contents: int = 500,    # ä½¿ç”¨çš„å†…å®¹æ•°é‡ (åŒ¹é…å­—ç¬¦æ•°)
    train_samples: int = 50000, # è®­ç»ƒæ ·æœ¬æ•°
    batch_size: int = 32,       # æ‰¹é‡å¤§å°
    epochs: int = 50,           # è®­ç»ƒè½®æ•°
    lr: float = 5e-4,           # å­¦ä¹ ç‡
    
    # è¯„ä¼°å‚æ•°
    eval_samples: int = 5000,   # è¯„ä¼°æ ·æœ¬æ•°
):
    """
    è¿è¡Œå®Œæ•´æ•°æ®é›†è®­ç»ƒ
    """
    print("ğŸš€ å¼€å§‹å…¨æ•°æ®é›†å­—ä½“è®­ç»ƒ...")
    print(f"ğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"  - ä»»åŠ¡: {task}")
    print(f"  - ç¼–ç å™¨: {encoder_type}")  # ğŸ”¥ æ˜¾ç¤ºç¼–ç å™¨ç±»å‹
    print(f"  - è®¾å¤‡: {device}")
    print(f"  - é£æ ¼æ•°: {num_styles}")
    print(f"  - å†…å®¹æ•°: {num_contents}")
    print(f"  - è®­ç»ƒæ ·æœ¬: {train_samples}")
    print(f"  - æ‰¹é‡å¤§å°: {batch_size}")
    print(f"  - è®­ç»ƒè½®æ•°: {epochs}")
    
    # 1. åŠ è½½æ•°æ®
    print("\nğŸ“¥ åŠ è½½æ½œåœ¨ç¼–ç æ•°æ®...")
    print(f"  - PTæ–‡ä»¶è·¯å¾„: {pt_path}")
    if chars_path:
        print(f"  - å­—ç¬¦æ–‡ä»¶è·¯å¾„: {chars_path}")
    if fonts_json:
        print(f"  - å­—ä½“JSONè·¯å¾„: {fonts_json}")
    
    accessor = ImprovedLatentAccessor(
        pt_path=pt_path,
        chars_path=chars_path,
        fonts_json=fonts_json,
        device="cpu",  # å…ˆåœ¨CPUä¸ŠåŠ è½½
        latent_shape=(4, 16, 16)
    )
    
    # 2. åŠ è½½VAEè§£ç å™¨
    print("\nğŸ”§ åŠ è½½VAEè§£ç å™¨...")
    decoder, cfg, device_used = load_vae_decoder(device=device)
    print(f"[INFO] VAE Decoder loaded on {device_used}")
    
    # 3. åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
    print("\nğŸ“š åˆ›å»ºè®­ç»ƒæ•°æ®é›†...")
    train_dataset = FullDatasetPairDataset(
        accessor=accessor,
        decoder=decoder,
        cfg=cfg,
        device_used=device_used,
        task=task,
        num_styles=num_styles,
        num_contents=num_contents,
        length=train_samples,
        augment=True,
        device="cpu"  # åœ¨datasetä¸­ä¿æŒCPUï¼Œåœ¨è®­ç»ƒæ—¶ç§»åˆ°GPU
    )
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        num_workers=0,  # ç¦ç”¨å¤šè¿›ç¨‹é¿å…å­˜å‚¨å†²çª
        pin_memory=True if device == "cuda" else False
    )
    
    # 4. åˆ›å»ºæ¨¡å‹
    print("\nğŸ¤– åˆ›å»ºSiameseæ¨¡å‹...")
    # ä»ä¸€ä¸ªæ ·æœ¬æ¨æ–­é€šé“æ•°
    sample_img = decode_to_image(decoder, accessor.get(0, 0), cfg, device_used)
    C = sample_img.shape[0]
    print(f"  - è¾“å…¥é€šé“æ•°: {C}")
    print(f"  - ä»»åŠ¡ç±»å‹: {task}")
    print(f"  - ç¼–ç å™¨ç±»å‹: {encoder_type}")
    
    model = SiameseJudge(
        in_ch=C, 
        emb_dim=512, 
        mlp_hidden=512, 
        task=task,
        encoder_type=encoder_type  # ğŸ”¥ ä¼ é€’ç¼–ç å™¨ç±»å‹
    )
    
    # 5. è®­ç»ƒæ¨¡å‹
    print("\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
    model = train_full_model(model, train_loader, device=device, lr=lr, epochs=epochs, task=task)
    
    # 6. è¯„ä¼°æ¨¡å‹
    print("\nğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    
    # ğŸ”¥ å…ˆè¿›è¡Œå¿«é€Ÿç†æ™ºæ£€æŸ¥
    sanity_ok = quick_sanity_check(model, accessor, decoder, cfg, device_used, task, device, num_samples=50)
    
    if sanity_ok:
        # å¦‚æœç†æ™ºæ£€æŸ¥é€šè¿‡ï¼Œå†è¿›è¡Œè¯¦ç»†è¯„ä¼°
        accuracy = eval_model(model, accessor, decoder, cfg, device_used, task, num_styles, num_contents, eval_samples, device)
    else:
        print("âš ï¸  è·³è¿‡è¯¦ç»†è¯„ä¼°ï¼Œæ¨¡å‹éœ€è¦è°ƒè¯•")
        accuracy = 0.0
    
    return model, accessor, decoder

class WeightedFocalLoss(nn.Module):
    """åŠ æƒFocal Loss - å¯¹å›°éš¾æ ·æœ¬ç»™äºˆæ›´é«˜æƒé‡"""
    def __init__(self, alpha=1, gamma=2, pos_weight=1.0, neg_weight=1.0):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.pos_weight = pos_weight  # æ­£æ ·æœ¬æƒé‡
        self.neg_weight = neg_weight  # è´Ÿæ ·æœ¬æƒé‡
        
    def forward(self, inputs, targets):
        # ç¡®ä¿inputsæ˜¯æ­£ç¡®çš„tensoræ ¼å¼
        if not isinstance(inputs, torch.Tensor):
            raise TypeError(f"Expected tensor, got {type(inputs)}")
        
        # è®¡ç®—sigmoidæ¦‚ç‡
        probs = torch.sigmoid(inputs)
        
        # è®¡ç®—åŸºç¡€BCEæŸå¤±
        ce_loss = F.binary_cross_entropy(probs, targets, reduction='none')
        
        # è®¡ç®—pt (é¢„æµ‹æ­£ç¡®çš„æ¦‚ç‡)
        pt = torch.where(targets == 1, probs, 1 - probs)
        
        # Focal weight: (1-pt)^gamma
        focal_weight = (1 - pt) ** self.gamma
        
        # ç±»åˆ«æƒé‡
        class_weight = torch.where(targets == 1, self.pos_weight, self.neg_weight)
        
        # æœ€ç»ˆæŸå¤±
        focal_loss = self.alpha * class_weight * focal_weight * ce_loss
        return focal_loss.mean()

class AdaptiveLoss(nn.Module):
    """è‡ªé€‚åº”æŸå¤± - æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒæ•´æƒé‡"""
    def __init__(self, task="content"):
        super().__init__()
        if task == "content":
            # Contentä»»åŠ¡: è´Ÿæ ·æœ¬(ä¸åŒå†…å®¹ç›¸åŒé£æ ¼)æ›´éš¾ï¼Œç»™æ›´é«˜æƒé‡
            # æ ¹æ®ä¹‹å‰ç»“æœåˆ†æï¼Œè´Ÿæ ·æœ¬å‡†ç¡®ç‡åä½(66.2%)ï¼Œéœ€è¦åŠ å¼º
            self.pos_weight = 1.0
            self.neg_weight = 3.0  # ğŸ”¥ æ˜¾è‘—å¢åŠ è´Ÿæ ·æœ¬æƒé‡
        else:  # style
            # Styleä»»åŠ¡: æ­£æ ·æœ¬(ç›¸åŒé£æ ¼ä¸åŒå†…å®¹)æ›´éš¾ï¼Œç»™æ›´é«˜æƒé‡
            # æ ¹æ®ä¹‹å‰ç»“æœåˆ†æï¼Œæ­£æ ·æœ¬å‡†ç¡®ç‡åä½(63.9%)ï¼Œéœ€è¦åŠ å¼º  
            self.pos_weight = 2.5  # ğŸ”¥ å¢åŠ æ­£æ ·æœ¬æƒé‡
            self.neg_weight = 1.0
            
        self.focal_loss = WeightedFocalLoss(
            alpha=1.5, gamma=2,  # ğŸ”¥ å¢åŠ alphaå€¼
            pos_weight=self.pos_weight, 
            neg_weight=self.neg_weight
        )
        
    def forward(self, inputs, targets):
        return self.focal_loss(inputs, targets)

def train_full_model(model, loader, device="cuda", lr=1e-4, epochs=20, task="content"):
    """è®­ç»ƒå®Œæ•´æ¨¡å‹ - ä½¿ç”¨æ”¹è¿›çš„æŸå¤±å‡½æ•°"""
    model = model.to(device)
    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)
    
    # ğŸ”¥ ä½¿ç”¨è‡ªé€‚åº”æŸå¤±å‡½æ•°
    crit = AdaptiveLoss(task=task)
    
    for ep in range(1, epochs+1):
        model.train()
        tot_loss = 0.0
        tot_acc = 0.0
        num_batches = 0
        
        for i, (x1, x2, y) in enumerate(loader):
            x1, x2, y = x1.to(device), x2.to(device), y.to(device)
            
            opt.zero_grad()
            logits = model(x1, x2)  # ğŸ”¥ ç°åœ¨æ¨¡å‹åªè¿”å›logits
            
            # ğŸ”¥ ä½¿ç”¨æ”¹è¿›çš„æŸå¤±å‡½æ•°
            loss = crit(logits, y)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªé˜²æ­¢çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            opt.step()
            
            # ç»Ÿè®¡
            with torch.no_grad():
                pred = (torch.sigmoid(logits) > 0.5).float()
                acc = (pred == y).float().mean()
                tot_loss += loss.item()
                tot_acc += acc.item()
                num_batches += 1
            
            # æ‰“å°è¿›åº¦
            if (i + 1) % 100 == 0:
                avg_loss = tot_loss / num_batches
                avg_acc = tot_acc / num_batches
                print(f"  Batch {i+1}/{len(loader)}: loss={avg_loss:.4f}, acc={avg_acc:.3f}")
        
        scheduler.step()
        
        # Epochæ€»ç»“
        avg_loss = tot_loss / num_batches
        avg_acc = tot_acc / num_batches
        lr_current = scheduler.get_last_lr()[0]
        
        print(f"[Epoch {ep}/{epochs}] loss={avg_loss:.4f}, acc={avg_acc:.3f}, lr={lr_current:.2e}")
    
    return model

def eval_model(model, accessor, decoder, cfg, device_used, task, num_styles, num_contents, eval_samples, device):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½ - æ”¹è¿›ç‰ˆæœ¬"""
    model.eval()
    correct = 0
    total = 0
    pos_correct = 0
    neg_correct = 0
    pos_total = 0
    neg_total = 0
    
    print(f"ğŸ” è¯„ä¼° {eval_samples} ä¸ªæ ·æœ¬...")
    
    # ç”¨äºè¯„ä¼°çš„æµ‹è¯•é›†åº”è¯¥ä¸è®­ç»ƒé›†ä¸é‡å 
    test_contents = list(range(num_contents//2, num_contents))  # ä½¿ç”¨ååŠéƒ¨åˆ†å†…å®¹ä½œä¸ºæµ‹è¯•
    test_styles = list(range(num_styles//2, num_styles))      # ä½¿ç”¨ååŠéƒ¨åˆ†é£æ ¼ä½œä¸ºæµ‹è¯•
    
    with torch.no_grad():
        for eval_idx in range(eval_samples):
            # éšæœºé€‰æ‹©æµ‹è¯•æ ·æœ¬ï¼ˆä½¿ç”¨æµ‹è¯•é›†èŒƒå›´ï¼‰
            i = random.choice(test_contents)
            j = random.choice(test_contents)
            p = random.choice(test_styles)
            q = random.choice(test_styles)
            
            while j == i:
                j = random.choice(test_contents)
            while q == p:
                q = random.choice(test_styles)
            
            try:
                # ç”Ÿæˆæµ‹è¯•å›¾åƒ
                ci_sp = decode_to_image(decoder, accessor.get(i, p), cfg, device_used).to(device)
                ci_sq = decode_to_image(decoder, accessor.get(i, q), cfg, device_used).to(device)
                cj_sp = decode_to_image(decoder, accessor.get(j, p), cfg, device_used).to(device)
                
                if task == "content":
                    # æµ‹è¯•æ­£æ ·æœ¬: ç›¸åŒå†…å®¹
                    pos_prob = score_pair(model, ci_sp, ci_sq, device=device)
                    # æµ‹è¯•è´Ÿæ ·æœ¬: ä¸åŒå†…å®¹
                    neg_prob = score_pair(model, ci_sp, cj_sp, device=device)
                else:  # style
                    # æµ‹è¯•æ­£æ ·æœ¬: ç›¸åŒé£æ ¼
                    pos_prob = score_pair(model, ci_sp, cj_sp, device=device)
                    # æµ‹è¯•è´Ÿæ ·æœ¬: ä¸åŒé£æ ¼
                    neg_prob = score_pair(model, ci_sp, ci_sq, device=device)
                
                # æ£€æŸ¥åˆ†ç±»æ˜¯å¦æ­£ç¡®
                if pos_prob > 0.5:
                    pos_correct += 1
                pos_total += 1
                
                if neg_prob < 0.5:
                    neg_correct += 1
                neg_total += 1
                
                total += 2
                correct = pos_correct + neg_correct
                
            except Exception as e:
                continue
    
    # è®¡ç®—è¯¦ç»†ç»Ÿè®¡
    accuracy = correct / total if total > 0 else 0
    pos_accuracy = pos_correct / pos_total if pos_total > 0 else 0
    neg_accuracy = neg_correct / neg_total if neg_total > 0 else 0
    
    print(f"ğŸ“Š è¯„ä¼°å®Œæˆ:")
    print(f"   æ€»ä½“å‡†ç¡®ç‡: {accuracy:.3f} ({correct}/{total})")
    print(f"   æ­£æ ·æœ¬å‡†ç¡®ç‡: {pos_accuracy:.3f} ({pos_correct}/{pos_total})")
    print(f"   è´Ÿæ ·æœ¬å‡†ç¡®ç‡: {neg_accuracy:.3f} ({neg_correct}/{neg_total})")
    
    # ğŸ”¥ æ·»åŠ å¿«é€Ÿåˆ¤æ–­è®­ç»ƒæ•ˆæœçš„æŒ‡æ ‡
    if accuracy > 0.7:
        print("âœ… æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼å¯ä»¥è€ƒè™‘æ‰©å¤§æ•°æ®é›†")
    elif accuracy > 0.6:
        print("âš ï¸  æ¨¡å‹è¡¨ç°ä¸€èˆ¬ï¼Œå»ºè®®è°ƒæ•´è¶…å‚æ•°")
    else:
        print("âŒ æ¨¡å‹è¡¨ç°è¾ƒå·®ï¼Œéœ€è¦æ£€æŸ¥ä»£ç é€»è¾‘")
    
    return accuracy

def quick_sanity_check(model, accessor, decoder, cfg, device_used, task, device, num_samples=50):
    """
    å¿«é€Ÿç†æ™ºæ£€æŸ¥ - éªŒè¯æ¨¡å‹æ˜¯å¦å­¦åˆ°åŸºæœ¬ç‰¹å¾
    """
    print(f"\nğŸ” å¿«é€ŸéªŒè¯æ¨¡å‹ ({task})...")
    model.eval()
    
    correct = 0
    total = 0
    same_probs = []
    diff_probs = []
    
    with torch.no_grad():
        for sample_idx in range(min(num_samples, 10)):  # å…ˆåªæµ‹è¯•10ä¸ªæ ·æœ¬è¿›è¡Œè°ƒè¯•
            try:
                # ğŸ”¥ ä¿®å¤ï¼šæ¯æ¬¡ä½¿ç”¨ä¸åŒçš„æ ·æœ¬è¿›è¡Œæµ‹è¯•
                i = sample_idx % min(20, accessor.num_characters)  # å¾ªç¯ä½¿ç”¨ä¸åŒå†…å®¹
                p = sample_idx % min(20, accessor.num_styles_per_char)  # å¾ªç¯ä½¿ç”¨ä¸åŒé£æ ¼
                
                # ğŸ”¥ æ­£ç¡®çš„ä»»åŠ¡ç›¸å…³é‡‡æ ·ç­–ç•¥
                if task == "content":
                    # å†…å®¹ä»»åŠ¡ï¼šç›¸åŒå†…å®¹ä¸åŒé£æ ¼ = ç›¸ä¼¼, ä¸åŒå†…å®¹ç›¸åŒé£æ ¼ = ä¸ç›¸ä¼¼
                    q = (p + 5) % min(20, accessor.num_styles_per_char)  # ä¸åŒé£æ ¼
                    j = (i + 5) % min(20, accessor.num_characters)       # ä¸åŒå†…å®¹
                    
                    # ç›¸åŒæ ·æœ¬ï¼šç›¸åŒå†…å®¹ï¼Œä¸åŒé£æ ¼ (åº”è¯¥ç›¸ä¼¼)
                    img1 = decode_to_image(decoder, accessor.get(i, p), cfg, device_used).to(device)
                    img2 = decode_to_image(decoder, accessor.get(i, q), cfg, device_used).to(device)  # ç›¸åŒå†…å®¹iï¼Œä¸åŒé£æ ¼q
                    same_prob = score_pair(model, img1, img2, device=device)
                    same_probs.append(same_prob)
                    
                    # ä¸åŒæ ·æœ¬ï¼šä¸åŒå†…å®¹ï¼Œç›¸åŒé£æ ¼ (åº”è¯¥ä¸ç›¸ä¼¼)
                    img3 = decode_to_image(decoder, accessor.get(j, p), cfg, device_used).to(device)  # ä¸åŒå†…å®¹jï¼Œç›¸åŒé£æ ¼p
                    diff_prob = score_pair(model, img1, img3, device=device)
                    diff_probs.append(diff_prob)
                    
                else:  # style task
                    # é£æ ¼ä»»åŠ¡ï¼šç›¸åŒé£æ ¼ä¸åŒå†…å®¹ = ç›¸ä¼¼, ä¸åŒé£æ ¼ç›¸åŒå†…å®¹ = ä¸ç›¸ä¼¼
                    q = (p + 5) % min(20, accessor.num_styles_per_char)  # ä¸åŒé£æ ¼
                    j = (i + 5) % min(20, accessor.num_characters)       # ä¸åŒå†…å®¹
                    
                    # ç›¸åŒæ ·æœ¬ï¼šç›¸åŒé£æ ¼ï¼Œä¸åŒå†…å®¹ (åº”è¯¥ç›¸ä¼¼)
                    img1 = decode_to_image(decoder, accessor.get(i, p), cfg, device_used).to(device)
                    img2 = decode_to_image(decoder, accessor.get(j, p), cfg, device_used).to(device)  # ä¸åŒå†…å®¹jï¼Œç›¸åŒé£æ ¼p
                    same_prob = score_pair(model, img1, img2, device=device)
                    same_probs.append(same_prob)
                    
                    # ä¸åŒæ ·æœ¬ï¼šä¸åŒé£æ ¼ï¼Œç›¸åŒå†…å®¹ (åº”è¯¥ä¸ç›¸ä¼¼)
                    img3 = decode_to_image(decoder, accessor.get(i, q), cfg, device_used).to(device)  # ç›¸åŒå†…å®¹iï¼Œä¸åŒé£æ ¼q
                    diff_prob = score_pair(model, img1, img3, device=device)
                    diff_probs.append(diff_prob)
                
                # ğŸ”¥ æ·»åŠ è°ƒè¯•ä¿¡æ¯
                if sample_idx < 3:  # å‰3ä¸ªæ ·æœ¬è¯¦ç»†è°ƒè¯•
                    if task == "content":
                        print(f"  [è°ƒè¯•] æ ·æœ¬{sample_idx}: ç›¸åŒå†…å®¹({i},{p})vs({i},{q}) | ä¸åŒå†…å®¹({i},{p})vs({j},{p})")
                    else:
                        print(f"  [è°ƒè¯•] æ ·æœ¬{sample_idx}: ç›¸åŒé£æ ¼({i},{p})vs({j},{p}) | ä¸åŒé£æ ¼({i},{p})vs({i},{q})")
                    
                    print(f"  [è°ƒè¯•] ç›¸åŒæ ·æœ¬æ¦‚ç‡={same_prob:.4f}, ä¸åŒæ ·æœ¬æ¦‚ç‡={diff_prob:.4f}")
                    
                    # æ£€æŸ¥å›¾åƒå·®å¼‚
                    if task == "content":
                        img_diff_same = torch.abs(img1 - img2).max().item()
                        img_diff_diff = torch.abs(img1 - img3).max().item()
                        print(f"  [è°ƒè¯•] ç›¸åŒå†…å®¹å›¾åƒå·®å¼‚: {img_diff_same:.6f}")
                        print(f"  [è°ƒè¯•] ä¸åŒå†…å®¹å›¾åƒå·®å¼‚: {img_diff_diff:.6f}")
                        
                        # æ£€æŸ¥ç‰¹å¾
                        v1 = model.encoder(img1.unsqueeze(0))
                        v2 = model.encoder(img2.unsqueeze(0)) 
                        v3 = model.encoder(img3.unsqueeze(0))
                        
                        feature_diff_same = torch.abs(v1 - v2).max().item()
                        feature_diff_diff = torch.abs(v1 - v3).max().item()
                        
                        print(f"  [è°ƒè¯•] ç›¸åŒå†…å®¹ç‰¹å¾å·®å¼‚: {feature_diff_same:.6f}")
                        print(f"  [è°ƒè¯•] ä¸åŒå†…å®¹ç‰¹å¾å·®å¼‚: {feature_diff_diff:.6f}")
                        
                        # æ£€æŸ¥logit
                        logit_same = model(img1.unsqueeze(0), img2.unsqueeze(0))
                        logit_diff = model(img1.unsqueeze(0), img3.unsqueeze(0))
                        
                    else:  # style task
                        img_diff_same = torch.abs(img1 - img2).max().item() 
                        img_diff_diff = torch.abs(img1 - img3).max().item()
                        print(f"  [è°ƒè¯•] ç›¸åŒé£æ ¼å›¾åƒå·®å¼‚: {img_diff_same:.6f}")
                        print(f"  [è°ƒè¯•] ä¸åŒé£æ ¼å›¾åƒå·®å¼‚: {img_diff_diff:.6f}")
                        
                        # æ£€æŸ¥ç‰¹å¾
                        v1 = model.encoder(img1.unsqueeze(0))
                        v2 = model.encoder(img2.unsqueeze(0))
                        v3 = model.encoder(img3.unsqueeze(0))
                        
                        feature_diff_same = torch.abs(v1 - v2).max().item()
                        feature_diff_diff = torch.abs(v1 - v3).max().item()
                        
                        print(f"  [è°ƒè¯•] ç›¸åŒé£æ ¼ç‰¹å¾å·®å¼‚: {feature_diff_same:.6f}")
                        print(f"  [è°ƒè¯•] ä¸åŒé£æ ¼ç‰¹å¾å·®å¼‚: {feature_diff_diff:.6f}")
                        
                        # æ£€æŸ¥logit
                        logit_same = model(img1.unsqueeze(0), img2.unsqueeze(0))
                        logit_diff = model(img1.unsqueeze(0), img3.unsqueeze(0))
                    
                    print(f"  [è°ƒè¯•] ç›¸åŒæ ·æœ¬logit: {logit_same.item():.4f}")
                    print(f"  [è°ƒè¯•] ä¸åŒæ ·æœ¬logit: {logit_diff.item():.4f}")
                
                # ç›¸åŒæ ·æœ¬åº”è¯¥ç›¸ä¼¼åº¦é«˜ï¼Œä¸åŒæ ·æœ¬åº”è¯¥ç›¸ä¼¼åº¦ä½
                if same_prob > 0.8 and diff_prob < 0.5:
                    correct += 1
                total += 1
                
            except Exception as e:
                print(f"  [é”™è¯¯] æ ·æœ¬{sample_idx}å¤±è´¥: {e}")
                continue
    
    sanity_acc = correct / total if total > 0 else 0
    
    # ğŸ”¥ è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    if same_probs and diff_probs:
        avg_same = sum(same_probs) / len(same_probs)
        avg_diff = sum(diff_probs) / len(diff_probs)
        print(f"ğŸ§  ç†æ™ºæ£€æŸ¥è¯¦æƒ…:")
        print(f"   ç›¸åŒæ ·æœ¬å¹³å‡æ¦‚ç‡: {avg_same:.4f} (æœŸæœ›>0.8)")
        print(f"   ä¸åŒæ ·æœ¬å¹³å‡æ¦‚ç‡: {avg_diff:.4f} (æœŸæœ›<0.5)")
        print(f"   ç»¼åˆå‡†ç¡®ç‡: {sanity_acc:.3f}")
    else:
        print(f"ğŸ§  ç†æ™ºæ£€æŸ¥å‡†ç¡®ç‡: {sanity_acc:.3f}")
    
    if sanity_acc > 0.7:
        print("âœ… æ¨¡å‹å…·å¤‡åŸºæœ¬åˆ†è¾¨èƒ½åŠ›")
        return True
    elif avg_same < 0.1 and avg_diff < 0.1:
        print("âš ï¸  æ¨¡å‹è¾“å‡ºè¿‡å°ï¼Œå¯èƒ½æ˜¯sigmoidé¥±å’Œé—®é¢˜")
        return False
    elif abs(avg_same - avg_diff) < 0.1:
        print("âš ï¸  æ¨¡å‹æ— æ³•åŒºåˆ†ç›¸åŒå’Œä¸åŒæ ·æœ¬")
        return False
    else:
        print("âŒ æ¨¡å‹ç¼ºä¹åŸºæœ¬åˆ†è¾¨èƒ½åŠ›ï¼Œéœ€è¦æ£€æŸ¥")
        return False

def quick_debug_training(encoder_type="enhanced"):
    """
    å¿«é€Ÿè°ƒè¯•è®­ç»ƒ - ä½¿ç”¨å°æ•°æ®é›†éªŒè¯ä»£ç é€»è¾‘
    é€‚åˆå¿«é€Ÿè¿­ä»£å’Œè°ƒè¯•ï¼Œè®­ç»ƒæ—¶é—´çº¦5-10åˆ†é’Ÿ
    
    Args:
        encoder_type: ç¼–ç å™¨ç±»å‹ ("original", "enhanced", "vgg", "hybrid")
    """
    print(f"ğŸš€ å¿«é€Ÿè°ƒè¯•æ¨¡å¼ - å°æ•°æ®é›†è®­ç»ƒ (ç¼–ç å™¨: {encoder_type})")
    print("=" * 50)
    
    # ğŸ”¥ æå°çš„è°ƒè¯•é…ç½® - å¿«é€ŸéªŒè¯ä»£ç é€»è¾‘
    debug_config = {
        "num_styles": 20,        # åªç”¨20ç§é£æ ¼
        "num_contents": 50,      # åªç”¨50ç§å†…å®¹  
        "train_samples": 1000,   # åªè®­ç»ƒ1000ä¸ªæ ·æœ¬
        "batch_size": 16,        # è¾ƒå°çš„æ‰¹é‡å¤§å°
        "epochs": 5,             # åªè®­ç»ƒ5ä¸ªepoch
        "lr": 1e-3,              # è¾ƒé«˜çš„å­¦ä¹ ç‡å¿«é€Ÿæ”¶æ•›
        "eval_samples": 200      # å°‘é‡è¯„ä¼°æ ·æœ¬
    }
    
    print(f"ğŸ“Š è°ƒè¯•é…ç½®: {debug_config}")
    print("â±ï¸  é¢„è®¡è®­ç»ƒæ—¶é—´: 5-10åˆ†é’Ÿ")
    
    # è®­ç»ƒå†…å®¹åˆ†ç¦»æ¨¡å‹
    print("\nğŸ“ è®­ç»ƒå†…å®¹åˆ†ç¦»ä»»åŠ¡...")
    content_model, accessor, decoder = run_full_training(
        task="content",
        encoder_type=encoder_type,  # ğŸ”¥ ä¼ é€’ç¼–ç å™¨ç±»å‹
        **debug_config
    )
    
    # è®­ç»ƒé£æ ¼åˆ†ç¦»æ¨¡å‹  
    print("\nğŸ¨ è®­ç»ƒé£æ ¼åˆ†ç¦»ä»»åŠ¡...")
    style_model, _, _ = run_full_training(
        task="style", 
        encoder_type=encoder_type,  # ğŸ”¥ ä¼ é€’ç¼–ç å™¨ç±»å‹
        **debug_config
    )
    
    print("\nâœ… å¿«é€Ÿè°ƒè¯•è®­ç»ƒå®Œæˆ!")
    print("å¦‚æœç»“æœçœ‹èµ·æ¥åˆç†ï¼Œå¯ä»¥ä½¿ç”¨ full_scale_training() è¿›è¡Œå®Œæ•´è®­ç»ƒ")
    
    return content_model, style_model, accessor, decoder

def full_scale_training(encoder_type="enhanced"):
    """
    å®Œæ•´è§„æ¨¡è®­ç»ƒ - ä½¿ç”¨å…¨æ•°æ®é›†è·å¾—æœ€ä½³æ€§èƒ½
    è®­ç»ƒæ—¶é—´çº¦2-4å°æ—¶
    
    Args:
        encoder_type: ç¼–ç å™¨ç±»å‹ ("original", "enhanced", "vgg", "hybrid")
    """
    print(f"ğŸš€ å®Œæ•´è§„æ¨¡è®­ç»ƒ - å…¨æ•°æ®é›†è®­ç»ƒ (ç¼–ç å™¨: {encoder_type})")
    print("=" * 50)
    
    # ğŸ”¥ å…¨æ•°æ®é›†è®­ç»ƒé…ç½® - å……åˆ†åˆ©ç”¨æ•°æ®
    full_config = {
        "num_styles": 2000,      # ä½¿ç”¨2000ç§é£æ ¼
        "num_contents": 4000,    # ä½¿ç”¨4000ä¸ªå­—ç¬¦å†…å®¹
        "train_samples": 200000, # 20ä¸‡è®­ç»ƒæ ·æœ¬
        "batch_size": 64,        # è¾ƒå¤§çš„æ‰¹é‡å¤§å°
        "epochs": 100,            # å……åˆ†è®­ç»ƒ
        "lr": 1e-4,              # è¾ƒä½çš„å­¦ä¹ ç‡ç¨³å®šè®­ç»ƒ
        "eval_samples": 5000     # å……åˆ†çš„è¯„ä¼°æ ·æœ¬
    }
    
    print(f"ğŸ“Š å…¨æ•°æ®é›†é…ç½®: {full_config}")
    print("â±ï¸  é¢„è®¡è®­ç»ƒæ—¶é—´: 2-4å°æ—¶")
    print(f"ğŸ’¾ åˆ©ç”¨æ•°æ®è§„æ¨¡: {full_config['num_styles']} Ã— {full_config['num_contents']} = {full_config['num_styles'] * full_config['num_contents']:,} ç»„åˆ")
    
    # è¿è¡Œå†…å®¹ä»»åŠ¡è®­ç»ƒ
    print("\nğŸ“ è®­ç»ƒå†…å®¹åˆ†ç¦»ä»»åŠ¡...")
    content_model, accessor, decoder = run_full_training(
        task="content",
        encoder_type=encoder_type,
        **full_config
    )
    
    # ä¿å­˜å†…å®¹æ¨¡å‹
    torch.save(content_model.state_dict(), "content_siamese_model_full.pth")
    print("ğŸ’¾ å†…å®¹æ¨¡å‹å·²ä¿å­˜åˆ°: content_siamese_model_full.pth")
    
    # è¿è¡Œé£æ ¼ä»»åŠ¡è®­ç»ƒ
    print("\nğŸ¨ è®­ç»ƒé£æ ¼åˆ†ç¦»ä»»åŠ¡...")
    style_model, _, _ = run_full_training(
        task="style",
        encoder_type=encoder_type,
        **full_config
    )
    
    # ä¿å­˜é£æ ¼æ¨¡å‹
    torch.save(style_model.state_dict(), "style_siamese_model_full.pth")
    print("ğŸ’¾ é£æ ¼æ¨¡å‹å·²ä¿å­˜åˆ°: style_siamese_model_full.pth")
    
    print("\nğŸ‰ å®Œæ•´è§„æ¨¡è®­ç»ƒå®Œæˆ!")
    print("ğŸ“ˆ æ¨¡å‹å·²ä¿å­˜ï¼Œå¯ç”¨äºæ¨ç†å’Œè¿›ä¸€æ­¥åˆ†æ")
    
    return content_model, style_model, accessor, decoder

if __name__ == "__main__":
    import sys
    
    # ğŸ”¥ æ ¹æ®å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©è®­ç»ƒæ¨¡å¼
    if len(sys.argv) > 1 and sys.argv[1] == "debug":
        print("ğŸ” å¯åŠ¨å¿«é€Ÿè°ƒè¯•è®­ç»ƒ...")
        print("ğŸ’¡ æç¤º: ä½¿ç”¨ 'python full_training.py' è¿›è¡Œå®Œæ•´è®­ç»ƒ")
        quick_debug_training()
    else:
        print("ğŸ¯ å¯åŠ¨å®Œæ•´è§„æ¨¡è®­ç»ƒ...")
        print("ğŸ’¡ æç¤º: ä½¿ç”¨ 'python full_training.py debug' è¿›è¡Œå¿«é€Ÿè°ƒè¯•")
        full_scale_training()